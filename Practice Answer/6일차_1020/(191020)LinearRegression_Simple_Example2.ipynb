{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data.ndim =  2 , x_data.shape =  (25, 3)\n",
      "t_data.ndim =  2 , t_data.shape =  (25, 1)\n"
     ]
    }
   ],
   "source": [
    "loaded_data = np.loadtxt('./data-01.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "x_data = loaded_data[ :, 0:-1]\n",
    "t_data = loaded_data[ :, [-1]]\n",
    "\n",
    "# 데이터 차원 및 shape 확인\n",
    "print(\"x_data.ndim = \", x_data.ndim, \", x_data.shape = \", x_data.shape)\n",
    "print(\"t_data.ndim = \", t_data.ndim, \", t_data.shape = \", t_data.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W =  [[0.1180762 ]\n",
      " [0.62627809]\n",
      " [0.6607044 ]] , W.shape =  (3, 1) , b =  [0.5119886] , b.shape =  (1,)\n"
     ]
    }
   ],
   "source": [
    "W = np.random.rand(3,1)  # 3X1 행렬\n",
    "b = np.random.rand(1)  \n",
    "print(\"W = \", W, \", W.shape = \", W.shape, \", b = \", b, \", b.shape = \", b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(x, t):\n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return ( np.sum( (t - y)**2 ) ) / ( len(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 값 계산 함수\n",
    "# 입력변수 x, t : numpy type\n",
    "def loss_val(x, t):\n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return ( np.sum( (t - y)**2 ) ) / ( len(x) )\n",
    "\n",
    "\n",
    "# 학습을 마친 후, 임의의 데이터에 대해 미래 값 예측 함수\n",
    "# 입력변수 x : numpy type\n",
    "def predict(x):\n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss value =  2423.966431452459 Initial W =  [[0.1180762 ]\n",
      " [0.62627809]\n",
      " [0.6607044 ]] \n",
      " , b =  [0.5119886]\n",
      "step =  0 loss value =  902.287668435981 W =  [[0.19659955]\n",
      " [0.70510003]\n",
      " [0.74161331]] , b =  [0.51257901]\n",
      "step =  400 loss value =  8.595098820172003 W =  [[0.32088568]\n",
      " [0.78085495]\n",
      " [0.91021701]] , b =  [0.51301986]\n",
      "step =  800 loss value =  7.880719979267268 W =  [[0.32330348]\n",
      " [0.74155692]\n",
      " [0.94627588]] , b =  [0.5124969]\n",
      "step =  1200 loss value =  7.379180195711286 W =  [[0.32564141]\n",
      " [0.7084956 ]\n",
      " [0.9763183 ]] , b =  [0.51193557]\n",
      "step =  1600 loss value =  7.026944600693482 W =  [[0.32788259]\n",
      " [0.68067067]\n",
      " [1.00133807]] , b =  [0.51134232]\n",
      "step =  2000 loss value =  6.779464242017329 W =  [[0.33001577]\n",
      " [0.65724329]\n",
      " [1.0221658 ]] , b =  [0.51072251]\n",
      "step =  2400 loss value =  6.60549894501418 W =  [[0.33203408]\n",
      " [0.63750994]\n",
      " [1.0394956 ]] , b =  [0.51008064]\n",
      "step =  2800 loss value =  6.483138403276789 W =  [[0.33393411]\n",
      " [0.62088047]\n",
      " [1.05390744]] , b =  [0.50942044]\n",
      "step =  3200 loss value =  6.397013379782825 W =  [[0.33571509]\n",
      " [0.6068598 ]\n",
      " [1.06588583]] , b =  [0.50874503]\n",
      "step =  3600 loss value =  6.336340952747272 W =  [[0.3373783 ]\n",
      " [0.59503247]\n",
      " [1.07583546]] , b =  [0.50805702]\n",
      "step =  4000 loss value =  6.293554288567862 W =  [[0.33892653]\n",
      " [0.58504982]\n",
      " [1.08409434]] , b =  [0.50735856]\n",
      "step =  4400 loss value =  6.263342085927994 W =  [[0.34036368]\n",
      " [0.57661915]\n",
      " [1.09094466]] , b =  [0.50665147]\n",
      "step =  4800 loss value =  6.241975153588848 W =  [[0.34169441]\n",
      " [0.56949473]\n",
      " [1.09662202]] , b =  [0.50593725]\n",
      "step =  5200 loss value =  6.226834246559307 W =  [[0.34292391]\n",
      " [0.56347016]\n",
      " [1.10132304]] , b =  [0.50521715]\n",
      "step =  5600 loss value =  6.216078976718034 W =  [[0.34405769]\n",
      " [0.55837207]\n",
      " [1.10521179]] , b =  [0.5044922]\n",
      "step =  6000 loss value =  6.208415617784841 W =  [[0.34510139]\n",
      " [0.55405479]\n",
      " [1.10842513]] , b =  [0.50376327]\n",
      "step =  6400 loss value =  6.202934241394506 W =  [[0.34606068]\n",
      " [0.55039586]\n",
      " [1.11107717]] , b =  [0.50303109]\n",
      "step =  6800 loss value =  6.198994462972028 W =  [[0.34694117]\n",
      " [0.54729235]\n",
      " [1.11326309]] , b =  [0.50229624]\n",
      "step =  7200 loss value =  6.196145272761908 W =  [[0.34774833]\n",
      " [0.54465766]\n",
      " [1.11506215]] , b =  [0.50155923]\n",
      "step =  7600 loss value =  6.194068770271996 W =  [[0.34848743]\n",
      " [0.54241895]\n",
      " [1.11654043]] , b =  [0.50082045]\n",
      "step =  8000 loss value =  6.192540664228501 W =  [[0.34916353]\n",
      " [0.5405149 ]\n",
      " [1.11775293]] , b =  [0.50008026]\n",
      "step =  8400 loss value =  6.191402533581499 W =  [[0.34978143]\n",
      " [0.53889387]\n",
      " [1.11874542]] , b =  [0.49933893]\n",
      "step =  8800 loss value =  6.190542340524848 W =  [[0.35034567]\n",
      " [0.53751238]\n",
      " [1.119556  ]] , b =  [0.4985967]\n",
      "step =  9200 loss value =  6.189880734766026 W =  [[0.35086053]\n",
      " [0.53633377]\n",
      " [1.12021633]] , b =  [0.49785375]\n",
      "step =  9600 loss value =  6.189361423159183 W =  [[0.35132999]\n",
      " [0.53532712]\n",
      " [1.12075273]] , b =  [0.49711025]\n",
      "step =  10000 loss value =  6.188944394035138 W =  [[0.3517578 ]\n",
      " [0.53446636]\n",
      " [1.12118706]] , b =  [0.49636632]\n",
      "step =  10400 loss value =  6.188601146816027 W =  [[0.35214743]\n",
      " [0.53372947]\n",
      " [1.12153745]] , b =  [0.49562207]\n",
      "step =  10800 loss value =  6.188311330830381 W =  [[0.3525021 ]\n",
      " [0.53309787]\n",
      " [1.12181893]] , b =  [0.4948776]\n",
      "step =  11200 loss value =  6.188060374913116 W =  [[0.35282479]\n",
      " [0.53255583]\n",
      " [1.12204397]] , b =  [0.49413296]\n",
      "step =  11600 loss value =  6.187837814000892 W =  [[0.35311826]\n",
      " [0.53209007]\n",
      " [1.12222287]] , b =  [0.49338823]\n",
      "step =  12000 loss value =  6.187636106366556 W =  [[0.35338505]\n",
      " [0.53168933]\n",
      " [1.12236416]] , b =  [0.49264344]\n",
      "step =  12400 loss value =  6.187449796492237 W =  [[0.3536275 ]\n",
      " [0.53134409]\n",
      " [1.12247488]] , b =  [0.49189864]\n",
      "step =  12800 loss value =  6.187274921644796 W =  [[0.35384774]\n",
      " [0.53104627]\n",
      " [1.12256083]] , b =  [0.49115386]\n",
      "step =  13200 loss value =  6.187108590453464 W =  [[0.35404776]\n",
      " [0.53078901]\n",
      " [1.12262681]] , b =  [0.49040912]\n",
      "step =  13600 loss value =  6.186948683024774 W =  [[0.35422936]\n",
      " [0.53056649]\n",
      " [1.12267673]] , b =  [0.48966444]\n",
      "step =  14000 loss value =  6.186793637050613 W =  [[0.35439419]\n",
      " [0.53037378]\n",
      " [1.12271382]] , b =  [0.48891985]\n",
      "step =  14400 loss value =  6.186642294853117 W =  [[0.35454377]\n",
      " [0.53020665]\n",
      " [1.12274074]] , b =  [0.48817534]\n",
      "step =  14800 loss value =  6.186493793685335 W =  [[0.35467947]\n",
      " [0.53006153]\n",
      " [1.12275963]] , b =  [0.48743094]\n",
      "step =  15200 loss value =  6.1863474867991775 W =  [[0.35480257]\n",
      " [0.52993536]\n",
      " [1.12277226]] , b =  [0.48668665]\n",
      "step =  15600 loss value =  6.186202886445294 W =  [[0.3549142 ]\n",
      " [0.52982553]\n",
      " [1.12278006]] , b =  [0.48594247]\n",
      "step =  16000 loss value =  6.186059622547771 W =  [[0.35501542]\n",
      " [0.52972981]\n",
      " [1.12278419]] , b =  [0.48519841]\n",
      "step =  16400 loss value =  6.185917412612646 W =  [[0.35510718]\n",
      " [0.52964629]\n",
      " [1.12278557]] , b =  [0.48445448]\n",
      "step =  16800 loss value =  6.185776039714048 W =  [[0.35519037]\n",
      " [0.52957335]\n",
      " [1.12278497]] , b =  [0.48371067]\n",
      "step =  17200 loss value =  6.185635336307978 W =  [[0.35526576]\n",
      " [0.52950957]\n",
      " [1.12278297]] , b =  [0.48296699]\n",
      "step =  17600 loss value =  6.185495172267926 W =  [[0.35533408]\n",
      " [0.52945376]\n",
      " [1.12278006]] , b =  [0.48222344]\n",
      "step =  18000 loss value =  6.185355445991324 W =  [[0.35539599]\n",
      " [0.52940488]\n",
      " [1.12277661]] , b =  [0.48148002]\n",
      "step =  18400 loss value =  6.185216077749971 W =  [[0.35545208]\n",
      " [0.52936205]\n",
      " [1.12277291]] , b =  [0.48073672]\n",
      "step =  18800 loss value =  6.185077004688767 W =  [[0.3555029 ]\n",
      " [0.52932448]\n",
      " [1.12276919]] , b =  [0.47999356]\n",
      "step =  19200 loss value =  6.184938177041312 W =  [[0.35554893]\n",
      " [0.52929152]\n",
      " [1.12276561]] , b =  [0.47925052]\n",
      "step =  19600 loss value =  6.184799555248738 W =  [[0.35559062]\n",
      " [0.52926259]\n",
      " [1.1227623 ]] , b =  [0.47850762]\n",
      "step =  20000 loss value =  6.184661107753349 W =  [[0.35562839]\n",
      " [0.5292372 ]\n",
      " [1.12275936]] , b =  [0.47776484]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-5  # 1e-2, 1e-3 은 손실함수 값 발산\n",
    "\n",
    "f = lambda x : loss_func(x_data,t_data)\n",
    "\n",
    "print(\"Initial loss value = \", loss_val(x_data, t_data), \"Initial W = \", W, \"\\n\", \", b = \", b )\n",
    "\n",
    "for step in  range(20001):  \n",
    "    \n",
    "    W -= learning_rate * numerical_derivative(f, W)\n",
    "    \n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "    \n",
    "    if (step % 400 == 0):\n",
    "        print(\"step = \", step, \"loss value = \", loss_val(x_data, t_data), \"W = \", W, \", b = \",b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([178.84935685])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([100, 98, 81])\n",
    "\n",
    "predict(test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
