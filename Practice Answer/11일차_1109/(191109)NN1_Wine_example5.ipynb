{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataGeneration ver1 이용한 training data / test data 생성 후,\n",
    "#### 생성된 training data / test data 에 포함된 target distribution 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# 수치미분 함수\n",
    "\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad\n",
    "\n",
    "# sigmoid 함수\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wine 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wine Class\n",
    "\n",
    "class Wine:\n",
    "    \n",
    "    # 생성자\n",
    "    # xdata, tdata => numpy.array(...)\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        \n",
    "        # 2층 hidden layer unit \n",
    "        # 가중치 W, 바이어스 b 초기화\n",
    "        self.W2 = np.random.rand(input_nodes, hidden_nodes)  \n",
    "        self.b2 = np.random.rand(hidden_nodes)\n",
    "        \n",
    "        # 3층 output layer unit : 1 개 \n",
    "        self.W3 = np.random.rand(hidden_nodes,output_nodes)\n",
    "        self.b3 = np.random.rand(output_nodes)\n",
    "                        \n",
    "        # 학습률 learning rate 초기화\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        print(\"Wine object is created !!!\")\n",
    "        \n",
    "    # 손실함수\n",
    "    def feed_forward(self):\n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "        z2 = np.dot(self.input_data, self.W2) + self.b2\n",
    "        a2 = sigmoid(z2)\n",
    "        \n",
    "        z3 = np.dot(a2, self.W3) + self.b3\n",
    "        y = a3 = sigmoid(z3)\n",
    "    \n",
    "        # cross-entropy \n",
    "        return  -np.sum( self.target_data*np.log(y + delta) + (1-self.target_data)*np.log((1 - y)+delta ) )\n",
    "    \n",
    "    # obtain W and b\n",
    "    def get_W_b(self):\n",
    "        \n",
    "        return self.W2,  self.b2, self.W3, self.b3\n",
    "    \n",
    "    # 손실 값 계산\n",
    "    def loss_val(self):\n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "        z2 = np.dot(self.input_data, self.W2) + self.b2\n",
    "        a2 = sigmoid(z2)\n",
    "        \n",
    "        z3 = np.dot(a2, self.W3) + self.b3\n",
    "        y = a3 = sigmoid(z3)\n",
    "    \n",
    "        # cross-entropy \n",
    "        return  -np.sum( self.target_data*np.log(y + delta) + (1-self.target_data)*np.log((1 - y)+delta ) )\n",
    "    \n",
    "    # query, 즉 미래 값 예측 함수\n",
    "    def predict(self, input_data):    \n",
    "        \n",
    "        z2 = np.dot(input_data, self.W2) + self.b2\n",
    "        a2 = sigmoid(z2)\n",
    "        \n",
    "        z3 = np.dot(a2, self.W3) + self.b3\n",
    "        y = a3 = sigmoid(z3)\n",
    "    \n",
    "        if y >= 0.5:\n",
    "            result = 1  # True\n",
    "        else:\n",
    "            result = 0  # False\n",
    "    \n",
    "        return y, result\n",
    "\n",
    "    \n",
    "    def accuracy(self, input_data, target_data):\n",
    "        \n",
    "        matched_list = []\n",
    "        not_matched_list = []\n",
    "        \n",
    "        # list which contains (index, label, prediction) value\n",
    "        index_label_prediction_list = []\n",
    "        \n",
    "        # temp list which contains label and prediction in sequence\n",
    "        temp_list = []\n",
    "        \n",
    "        for index in range(len(input_data)):\n",
    "            \n",
    "            (real_val, logical_val) = self.predict(input_data[index])\n",
    "            \n",
    "            if logical_val == target_data[index]:\n",
    "                matched_list.append(index)\n",
    "            else:\n",
    "                not_matched_list.append(index)\n",
    "                \n",
    "                temp_list.append(index)\n",
    "                temp_list.append(target_data[index])\n",
    "                temp_list.append(logical_val)\n",
    "                \n",
    "                index_label_prediction_list.append(temp_list)\n",
    "                \n",
    "                temp_list = []\n",
    "                \n",
    "                \n",
    "        accuracy_result = len(matched_list) / len(input_data)\n",
    "        \n",
    "        print(\"Accuracy => \", accuracy_result)\n",
    "        \n",
    "        return matched_list, not_matched_list, index_label_prediction_list\n",
    "    \n",
    "        \n",
    "    # 수치미분을 이용하여 손실함수가 최소가 될때 까지 학습하는 함수\n",
    "    def train(self, input_data, target_data):\n",
    "        \n",
    "        self.input_data = input_data\n",
    "        self.target_data = target_data\n",
    "        \n",
    "        f = lambda x : self.feed_forward()\n",
    "        \n",
    "        self.W2 -= self.learning_rate * numerical_derivative(f, self.W2)\n",
    "    \n",
    "        self.b2 -= self.learning_rate * numerical_derivative(f, self.b2)\n",
    "        \n",
    "        self.W3 -= self.learning_rate * numerical_derivative(f, self.W3)\n",
    "    \n",
    "        self.b3 -= self.learning_rate * numerical_derivative(f, self.b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataGeneration class 이용하여 training_data,  test_data 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneration:\n",
    "    \n",
    "    # target_position = 0 (첫번째열이 정답데이터), target_position=-1 (마지막열이 정답데이터)\n",
    "    def __init__(self, name, file_path, seperation_rate, target_position=-1):\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "        self.file_path = file_path\n",
    "        \n",
    "        self.seperation_rate = seperation_rate\n",
    "        \n",
    "        if (target_position == -1  or  target_position == 0):      \n",
    "            self.target_position = target_position\n",
    "        \n",
    "        else:\n",
    "            err_str = 'target_position must be -1 or 0'            \n",
    "            raise Exception(err_str)    \n",
    "            \n",
    "    \n",
    "    # print data target distribution \n",
    "    # str_of_kind : 'original data' or  'training data'  or  'test data'\n",
    "    def print_target_distribution(self, data, str_of_kind='original data'):\n",
    "        \n",
    "        print('=======================================================================================================')\n",
    "        \n",
    "        target_data = data[ :, self.target_position ]\n",
    "        \n",
    "        # numpy.unique() 사용하여 loaded data target 분포 확인\n",
    "        unique, counts = np.unique(target_data, return_counts=True)\n",
    "\n",
    "        print('[DataGeneration]  ', str_of_kind, ' target value = ', dict(zip(unique, counts)).items())\n",
    "\n",
    "        num_zeros = dict(zip(unique, counts))[0.0]  # key 0.0 에 대한 value 값 count 리턴\n",
    "        num_ones = dict(zip(unique, counts))[1.0]  # key 1.0 에 대한 value 값 count 리턴\n",
    "\n",
    "        print('[DataGeneration]  ', str_of_kind, ' zeros numbers = ', num_zeros, ', ratio = ', 100 * num_zeros / (data.shape[0]), ' %')\n",
    "        print('[DataGeneration]  ', str_of_kind, ' ones numbers = ', num_ones, ', ratio = ', 100 * num_ones / (data.shape[0]), '%') \n",
    "    \n",
    "        print('=======================================================================================================')\n",
    "        \n",
    "        \n",
    "    # shuffle 기능을 이용하여 training_data / test_data 생성\n",
    "    def generate(self):\n",
    "    \n",
    "        # 데이터 불러오기, 파일이 없는 경우 exception 발생\n",
    "\n",
    "        try:\n",
    "            loaded_data = np.loadtxt(self.file_path, delimiter=',', dtype=np.float32)\n",
    "            \n",
    "        except Exception as err:\n",
    "            print('[DataGeneration::generate()]  ', str(err))\n",
    "            raise Exception(str(err))\n",
    "\n",
    "        print(\"[DataGeneration]  loaded_data.shape = \", loaded_data.shape)\n",
    "            \n",
    "        # print the target distribution of original data \n",
    "        \n",
    "        self.print_target_distribution(loaded_data, 'original data')\n",
    "        \n",
    "        \n",
    "        # random.shuffle() 이용한 데이터 인덱스 분리 및 트레이닝/테스트 데이터 생성\n",
    "        \n",
    "        # 임시 저장 리스트\n",
    "        training_data_list = []\n",
    "        test_data_list = []\n",
    "\n",
    "        # 분리비율에 맞게 테스트데이터로 분리\n",
    "        total_data_num = len(loaded_data)\n",
    "        test_data_num = int(len(loaded_data) * self.seperation_rate)\n",
    "\n",
    "        #print(\"[DataGeneration]  total_data_num = \", total_data_num, \", test_data_num = \", test_data_num)\n",
    "\n",
    "        # 전체 데이터 인덱스를 가지고 있는 리스트 생성\n",
    "        total_data_index_list = [ index for index in range(total_data_num) ]\n",
    "\n",
    "        # random.shuffle 을 이용하여 인덱스 리스트 생성\n",
    "        random.shuffle(total_data_index_list)  # 전체 인덱스가 랜덤하게 섞여진 리스트로 변형된다\n",
    "\n",
    "        # test data 를 위한 인덱스는 total_data_index_list 로뷰터 앞에서 분리비율(seperation_rate)의 데이터 인덱스\n",
    "        test_data_index_list = total_data_index_list[ 0:test_data_num ]\n",
    "\n",
    "        #print(\"[DataGeneration]  length of test_data_index_list = \", len(test_data_index_list))\n",
    "\n",
    "        # training data 를 위한 인덱스는 total_data_index_list 에서 test data 인덱스를 제외한 나머지 부분\n",
    "        training_data_index_list = total_data_index_list[ test_data_num: ]\n",
    "\n",
    "        #print(\"[DataGeneration]  length of training_data_index_list = \", len(training_data_index_list))\n",
    "\n",
    "        # training data 구성\n",
    "        for training_data_index in training_data_index_list:\n",
    "    \n",
    "            training_data_list.append(loaded_data[training_data_index])\n",
    "\n",
    "        # test data 구성\n",
    "        for test_data_index in test_data_index_list:\n",
    "    \n",
    "            test_data_list.append(loaded_data[test_data_index])\n",
    "\n",
    "        # generate training data from training_data_list using np.arrya(...)\n",
    "        training_data = np.array(training_data_list)\n",
    "\n",
    "        # generate test data from test_data_list using np.arrya(...)\n",
    "        test_data = np.array(test_data_list)\n",
    "\n",
    "        # verification shape\n",
    "        #print(\"[DataGeneration]  training_data.shape = \", training_data.shape)\n",
    "        #print(\"[DataGeneration]  test_data.shape = \", test_data.shape)\n",
    "\n",
    "        # print target distribution of generated data \n",
    "        \n",
    "        self.print_target_distribution(training_data, 'training data')\n",
    "        \n",
    "        self.print_target_distribution(test_data, 'test data')\n",
    "        \n",
    "        \n",
    "        # save training & test data (.csv)\n",
    "        training_data_save_path = './' + self.name + '_training_data.csv'\n",
    "        test_data_save_path = './' + self.name + '_test_data.csv'\n",
    "        \n",
    "        # 저장공간이 없거나 파일 write 실패시 exception 발생\n",
    "        try:\n",
    "            np.savetxt(training_data_save_path, training_data, delimiter=',')\n",
    "            np.savetxt(test_data_save_path, test_data, delimiter=',')\n",
    "            \n",
    "        except Exception as err:\n",
    "            print('[DataGeneration::generate()]  ', str(err))\n",
    "            raise Exception(str(err))\n",
    "        \n",
    "        return training_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataGeneration 객체 생성.\n",
    "#### 원본데이터 및 생성된 training data / test data 에 대한 정답(target) 분포 확인 기능 추가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataGeneration]  loaded_data.shape =  (6497, 13)\n",
      "=======================================================================================================\n",
      "[DataGeneration]   original data  target value =  dict_items([(0.0, 4898), (1.0, 1599)])\n",
      "[DataGeneration]   original data  zeros numbers =  4898 , ratio =  75.38864091118978  %\n",
      "[DataGeneration]   original data  ones numbers =  1599 , ratio =  24.61135908881022 %\n",
      "=======================================================================================================\n",
      "=======================================================================================================\n",
      "[DataGeneration]   training data  target value =  dict_items([(0.0, 3432), (1.0, 1116)])\n",
      "[DataGeneration]   training data  zeros numbers =  3432 , ratio =  75.46174142480211  %\n",
      "[DataGeneration]   training data  ones numbers =  1116 , ratio =  24.53825857519789 %\n",
      "=======================================================================================================\n",
      "=======================================================================================================\n",
      "[DataGeneration]   test data  target value =  dict_items([(0.0, 1466), (1.0, 483)])\n",
      "[DataGeneration]   test data  zeros numbers =  1466 , ratio =  75.21806054386865  %\n",
      "[DataGeneration]   test data  ones numbers =  483 , ratio =  24.78193945613135 %\n",
      "=======================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# DataGeneration 객체 생성.\n",
    "# 생성된 training data / test data 에 대한 정답(target) 분포 확인\n",
    "\n",
    "seperation_rate = 0.3 # 분리비율\n",
    "target_position = -1\n",
    "\n",
    "try:    \n",
    "    # DataGeneration 이용한 training data / test data 분리\n",
    "    \n",
    "    data_obj = DataGeneration('Wine', './(191109)wine.csv', seperation_rate, target_position)\n",
    "\n",
    "    (training_data, test_data) = data_obj.generate()\n",
    "    \n",
    "except Exception as err:\n",
    "    \n",
    "    print('Exception Occur !!')\n",
    "    print(str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-Parameter 설정 및 train 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wine object is created !!!\n",
      "Neural Network Learning using Numerical Derivative...\n",
      "epochs =  0 loss value =  6.938813362363636\n",
      "epochs =  2 loss value =  5.845202373258166\n",
      "epochs =  4 loss value =  4.760329366838736\n",
      "epochs =  6 loss value =  3.7007805055203695\n",
      "epochs =  8 loss value =  2.708548838741864\n",
      "epochs =  10 loss value =  1.8646441471986142\n",
      "epochs =  12 loss value =  1.2524609627607834\n",
      "epochs =  14 loss value =  0.8730233612691779\n",
      "epochs =  16 loss value =  0.6542578974583587\n",
      "epochs =  18 loss value =  0.5267793196972503\n",
      "\n",
      "Elapsed Time =>  0:32:04.116542\n"
     ]
    }
   ],
   "source": [
    "#hyper-parameter\n",
    "i_nodes = training_data.shape[1] - 1    # input nodes 개수\n",
    "h1_nodes = 15  # hidden nodes 개수\n",
    "o_nodes = 1    # output nodes 개수\n",
    "lr = 1e-5      # learning rate\n",
    "epochs = 20   # 반복횟수\n",
    "\n",
    "# Wine 객체 생성\n",
    "obj = Wine(i_nodes, h1_nodes, o_nodes, lr)\n",
    "\n",
    "print(\"Neural Network Learning using Numerical Derivative...\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for step in range(epochs):\n",
    "    \n",
    "    for index in range(len(training_data)):\n",
    "        \n",
    "        input_data = training_data[index, 0:-1]\n",
    "        target_data = training_data[index, [-1]]\n",
    "        \n",
    "        obj.train(input_data, target_data)\n",
    "        \n",
    "    if (step % 2 == 0):\n",
    "        print(\"epochs = \", step, \"loss value = \", obj.loss_val())\n",
    "\n",
    "end_time = datetime.now()\n",
    "        \n",
    "print(\"\")\n",
    "print(\"Elapsed Time => \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =>  0.7521806054386865\n"
     ]
    }
   ],
   "source": [
    "test_input_data = test_data[ :, 0:-1 ]\n",
    "test_target_data = test_data[ :, -1 ]\n",
    "\n",
    "(true_list, false_list, index_label_prediction_list) = obj.accuracy(test_input_data, test_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 1.0, 0], [9, 1.0, 0], [22, 1.0, 0], [30, 1.0, 0], [33, 1.0, 0], [34, 1.0, 0], [35, 1.0, 0], [43, 1.0, 0], [45, 1.0, 0], [58, 1.0, 0], [66, 1.0, 0], [70, 1.0, 0], [72, 1.0, 0], [75, 1.0, 0], [79, 1.0, 0], [85, 1.0, 0], [89, 1.0, 0], [90, 1.0, 0], [91, 1.0, 0], [95, 1.0, 0], [98, 1.0, 0], [114, 1.0, 0], [120, 1.0, 0], [128, 1.0, 0], [129, 1.0, 0], [134, 1.0, 0], [136, 1.0, 0], [138, 1.0, 0], [142, 1.0, 0], [143, 1.0, 0], [144, 1.0, 0], [146, 1.0, 0], [149, 1.0, 0], [150, 1.0, 0], [153, 1.0, 0], [160, 1.0, 0], [170, 1.0, 0], [172, 1.0, 0], [174, 1.0, 0], [175, 1.0, 0], [180, 1.0, 0], [188, 1.0, 0], [189, 1.0, 0], [190, 1.0, 0], [194, 1.0, 0], [196, 1.0, 0], [199, 1.0, 0], [205, 1.0, 0], [207, 1.0, 0], [208, 1.0, 0], [218, 1.0, 0], [219, 1.0, 0], [220, 1.0, 0], [226, 1.0, 0], [235, 1.0, 0], [243, 1.0, 0], [245, 1.0, 0], [249, 1.0, 0], [252, 1.0, 0], [258, 1.0, 0], [261, 1.0, 0], [264, 1.0, 0], [270, 1.0, 0], [271, 1.0, 0], [273, 1.0, 0], [274, 1.0, 0], [283, 1.0, 0], [289, 1.0, 0], [290, 1.0, 0], [291, 1.0, 0], [292, 1.0, 0], [294, 1.0, 0], [295, 1.0, 0], [313, 1.0, 0], [316, 1.0, 0], [326, 1.0, 0], [331, 1.0, 0], [338, 1.0, 0], [339, 1.0, 0], [340, 1.0, 0], [345, 1.0, 0], [346, 1.0, 0], [349, 1.0, 0], [359, 1.0, 0], [364, 1.0, 0], [367, 1.0, 0], [371, 1.0, 0], [374, 1.0, 0], [375, 1.0, 0], [376, 1.0, 0], [380, 1.0, 0], [386, 1.0, 0], [395, 1.0, 0], [396, 1.0, 0], [397, 1.0, 0], [404, 1.0, 0], [407, 1.0, 0], [412, 1.0, 0], [413, 1.0, 0], [414, 1.0, 0], [423, 1.0, 0], [426, 1.0, 0], [427, 1.0, 0], [431, 1.0, 0], [434, 1.0, 0], [436, 1.0, 0], [438, 1.0, 0], [439, 1.0, 0], [443, 1.0, 0], [451, 1.0, 0], [453, 1.0, 0], [454, 1.0, 0], [458, 1.0, 0], [462, 1.0, 0], [466, 1.0, 0], [477, 1.0, 0], [479, 1.0, 0], [480, 1.0, 0], [484, 1.0, 0], [485, 1.0, 0], [486, 1.0, 0], [494, 1.0, 0], [511, 1.0, 0], [515, 1.0, 0], [518, 1.0, 0], [520, 1.0, 0], [523, 1.0, 0], [532, 1.0, 0], [534, 1.0, 0], [536, 1.0, 0], [539, 1.0, 0], [542, 1.0, 0], [545, 1.0, 0], [559, 1.0, 0], [562, 1.0, 0], [566, 1.0, 0], [567, 1.0, 0], [570, 1.0, 0], [572, 1.0, 0], [574, 1.0, 0], [575, 1.0, 0], [576, 1.0, 0], [581, 1.0, 0], [582, 1.0, 0], [589, 1.0, 0], [591, 1.0, 0], [595, 1.0, 0], [599, 1.0, 0], [601, 1.0, 0], [604, 1.0, 0], [605, 1.0, 0], [606, 1.0, 0], [610, 1.0, 0], [613, 1.0, 0], [617, 1.0, 0], [622, 1.0, 0], [625, 1.0, 0], [626, 1.0, 0], [627, 1.0, 0], [628, 1.0, 0], [629, 1.0, 0], [633, 1.0, 0], [642, 1.0, 0], [643, 1.0, 0], [646, 1.0, 0], [651, 1.0, 0], [656, 1.0, 0], [662, 1.0, 0], [665, 1.0, 0], [669, 1.0, 0], [671, 1.0, 0], [672, 1.0, 0], [673, 1.0, 0], [678, 1.0, 0], [679, 1.0, 0], [682, 1.0, 0], [683, 1.0, 0], [684, 1.0, 0], [691, 1.0, 0], [692, 1.0, 0], [701, 1.0, 0], [706, 1.0, 0], [711, 1.0, 0], [712, 1.0, 0], [715, 1.0, 0], [719, 1.0, 0], [721, 1.0, 0], [723, 1.0, 0], [728, 1.0, 0], [736, 1.0, 0], [746, 1.0, 0], [747, 1.0, 0], [749, 1.0, 0], [752, 1.0, 0], [754, 1.0, 0], [757, 1.0, 0], [758, 1.0, 0], [765, 1.0, 0], [772, 1.0, 0], [773, 1.0, 0], [774, 1.0, 0], [775, 1.0, 0], [776, 1.0, 0], [778, 1.0, 0], [783, 1.0, 0], [787, 1.0, 0], [791, 1.0, 0], [794, 1.0, 0], [803, 1.0, 0], [806, 1.0, 0], [816, 1.0, 0], [826, 1.0, 0], [827, 1.0, 0], [828, 1.0, 0], [833, 1.0, 0], [835, 1.0, 0], [840, 1.0, 0], [852, 1.0, 0], [853, 1.0, 0], [861, 1.0, 0], [865, 1.0, 0], [875, 1.0, 0], [876, 1.0, 0], [882, 1.0, 0], [883, 1.0, 0], [886, 1.0, 0], [888, 1.0, 0], [891, 1.0, 0], [893, 1.0, 0], [894, 1.0, 0], [898, 1.0, 0], [902, 1.0, 0], [904, 1.0, 0], [907, 1.0, 0], [909, 1.0, 0], [911, 1.0, 0], [915, 1.0, 0], [916, 1.0, 0], [919, 1.0, 0], [932, 1.0, 0], [934, 1.0, 0], [938, 1.0, 0], [939, 1.0, 0], [940, 1.0, 0], [946, 1.0, 0], [947, 1.0, 0], [949, 1.0, 0], [953, 1.0, 0], [965, 1.0, 0], [968, 1.0, 0], [969, 1.0, 0], [972, 1.0, 0], [974, 1.0, 0], [980, 1.0, 0], [985, 1.0, 0], [993, 1.0, 0], [1009, 1.0, 0], [1010, 1.0, 0], [1013, 1.0, 0], [1018, 1.0, 0], [1022, 1.0, 0], [1023, 1.0, 0], [1026, 1.0, 0], [1037, 1.0, 0], [1043, 1.0, 0], [1052, 1.0, 0], [1055, 1.0, 0], [1056, 1.0, 0], [1058, 1.0, 0], [1062, 1.0, 0], [1064, 1.0, 0], [1065, 1.0, 0], [1070, 1.0, 0], [1075, 1.0, 0], [1079, 1.0, 0], [1081, 1.0, 0], [1084, 1.0, 0], [1086, 1.0, 0], [1089, 1.0, 0], [1095, 1.0, 0], [1098, 1.0, 0], [1099, 1.0, 0], [1104, 1.0, 0], [1110, 1.0, 0], [1112, 1.0, 0], [1113, 1.0, 0], [1116, 1.0, 0], [1118, 1.0, 0], [1124, 1.0, 0], [1127, 1.0, 0], [1130, 1.0, 0], [1133, 1.0, 0], [1135, 1.0, 0], [1142, 1.0, 0], [1147, 1.0, 0], [1149, 1.0, 0], [1150, 1.0, 0], [1153, 1.0, 0], [1159, 1.0, 0], [1163, 1.0, 0], [1170, 1.0, 0], [1171, 1.0, 0], [1172, 1.0, 0], [1173, 1.0, 0], [1174, 1.0, 0], [1176, 1.0, 0], [1177, 1.0, 0], [1185, 1.0, 0], [1186, 1.0, 0], [1189, 1.0, 0], [1191, 1.0, 0], [1197, 1.0, 0], [1206, 1.0, 0], [1210, 1.0, 0], [1212, 1.0, 0], [1213, 1.0, 0], [1215, 1.0, 0], [1218, 1.0, 0], [1223, 1.0, 0], [1226, 1.0, 0], [1227, 1.0, 0], [1229, 1.0, 0], [1230, 1.0, 0], [1239, 1.0, 0], [1243, 1.0, 0], [1245, 1.0, 0], [1251, 1.0, 0], [1252, 1.0, 0], [1254, 1.0, 0], [1269, 1.0, 0], [1274, 1.0, 0], [1277, 1.0, 0], [1278, 1.0, 0], [1280, 1.0, 0], [1281, 1.0, 0], [1285, 1.0, 0], [1288, 1.0, 0], [1289, 1.0, 0], [1294, 1.0, 0], [1296, 1.0, 0], [1297, 1.0, 0], [1304, 1.0, 0], [1306, 1.0, 0], [1307, 1.0, 0], [1314, 1.0, 0], [1316, 1.0, 0], [1317, 1.0, 0], [1319, 1.0, 0], [1332, 1.0, 0], [1337, 1.0, 0], [1342, 1.0, 0], [1343, 1.0, 0], [1346, 1.0, 0], [1360, 1.0, 0], [1362, 1.0, 0], [1365, 1.0, 0], [1368, 1.0, 0], [1369, 1.0, 0], [1370, 1.0, 0], [1376, 1.0, 0], [1383, 1.0, 0], [1391, 1.0, 0], [1395, 1.0, 0], [1400, 1.0, 0], [1403, 1.0, 0], [1413, 1.0, 0], [1414, 1.0, 0], [1415, 1.0, 0], [1425, 1.0, 0], [1432, 1.0, 0], [1439, 1.0, 0], [1446, 1.0, 0], [1450, 1.0, 0], [1453, 1.0, 0], [1457, 1.0, 0], [1460, 1.0, 0], [1464, 1.0, 0], [1465, 1.0, 0], [1466, 1.0, 0], [1469, 1.0, 0], [1472, 1.0, 0], [1474, 1.0, 0], [1475, 1.0, 0], [1476, 1.0, 0], [1481, 1.0, 0], [1483, 1.0, 0], [1487, 1.0, 0], [1491, 1.0, 0], [1501, 1.0, 0], [1506, 1.0, 0], [1511, 1.0, 0], [1512, 1.0, 0], [1515, 1.0, 0], [1521, 1.0, 0], [1522, 1.0, 0], [1525, 1.0, 0], [1529, 1.0, 0], [1539, 1.0, 0], [1543, 1.0, 0], [1558, 1.0, 0], [1567, 1.0, 0], [1569, 1.0, 0], [1571, 1.0, 0], [1585, 1.0, 0], [1587, 1.0, 0], [1589, 1.0, 0], [1592, 1.0, 0], [1602, 1.0, 0], [1604, 1.0, 0], [1613, 1.0, 0], [1617, 1.0, 0], [1626, 1.0, 0], [1628, 1.0, 0], [1631, 1.0, 0], [1632, 1.0, 0], [1633, 1.0, 0], [1635, 1.0, 0], [1636, 1.0, 0], [1637, 1.0, 0], [1642, 1.0, 0], [1644, 1.0, 0], [1653, 1.0, 0], [1660, 1.0, 0], [1663, 1.0, 0], [1664, 1.0, 0], [1672, 1.0, 0], [1688, 1.0, 0], [1694, 1.0, 0], [1701, 1.0, 0], [1704, 1.0, 0], [1708, 1.0, 0], [1711, 1.0, 0], [1716, 1.0, 0], [1722, 1.0, 0], [1725, 1.0, 0], [1726, 1.0, 0], [1738, 1.0, 0], [1740, 1.0, 0], [1744, 1.0, 0], [1745, 1.0, 0], [1754, 1.0, 0], [1766, 1.0, 0], [1770, 1.0, 0], [1773, 1.0, 0], [1781, 1.0, 0], [1785, 1.0, 0], [1787, 1.0, 0], [1795, 1.0, 0], [1805, 1.0, 0], [1820, 1.0, 0], [1822, 1.0, 0], [1824, 1.0, 0], [1826, 1.0, 0], [1827, 1.0, 0], [1831, 1.0, 0], [1833, 1.0, 0], [1842, 1.0, 0], [1844, 1.0, 0], [1849, 1.0, 0], [1855, 1.0, 0], [1862, 1.0, 0], [1871, 1.0, 0], [1872, 1.0, 0], [1874, 1.0, 0], [1877, 1.0, 0], [1878, 1.0, 0], [1879, 1.0, 0], [1881, 1.0, 0], [1886, 1.0, 0], [1888, 1.0, 0], [1890, 1.0, 0], [1896, 1.0, 0], [1905, 1.0, 0], [1906, 1.0, 0], [1907, 1.0, 0], [1908, 1.0, 0], [1909, 1.0, 0], [1912, 1.0, 0], [1916, 1.0, 0], [1919, 1.0, 0], [1943, 1.0, 0], [1947, 1.0, 0], [1948, 1.0, 0]]\n",
      "\n",
      "false predicton number =  483\n"
     ]
    }
   ],
   "source": [
    "print(index_label_prediction_list)\n",
    "print('\\nfalse predicton number = ', len(index_label_prediction_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
