{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatically upgrade code to Tensorflow\n",
    "# 텐서플로 2.0으로 코드 업그레이드\n",
    "### 참고 : 텐서플로 공식 홈페이지\n",
    "#### (한글) : https://www.tensorflow.org/guide/upgrade?hl=ko\n",
    "#### (영문) : https://www.tensorflow.org/guide/upgrade?hl=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.x 일 경우 다음 명령어로 Tensorflow 2.0 버전으로 업그레이드 (설치 후 kernel 재시작 필요) \n",
    "#### 명령어 : !pip install --upgrade tensorflow\n",
    "### <-> 2.0에서 1.15.0 버전으로 다운그레이드 할 경우 다음 명령어 사용.\n",
    "#### 명령어 : !pip install --upgrade tensorflow==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/72/6b3264aa2889b7dde7663464b99587d95cd6a5f3b9b30181f14d78a63e64/tensorflow-2.0.0-cp37-cp37m-macosx_10_11_x86_64.whl (102.7MB)\n",
      "\u001b[K     |████████████████████████████████| 102.7MB 3.8MB/s eta 0:00:01    |█████▎                          | 16.8MB 3.3MB/s eta 0:00:27     |█████████████████████▌          | 69.0MB 14.9MB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /Users/jihyunkang/anaconda3/envs/tf_1_15_0_new/lib/python3.7/site-packages (from tensorflow) (1.25.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /Users/jihyunkang/anaconda3/envs/tf_1_15_0_new/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /Users/jihyunkang/anaconda3/envs/tf_1_15_0_new/lib/python3.7/site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /Users/jihyunkang/anaconda3/envs/tf_1_15_0_new/lib/python3.7/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /Users/jihyunkang/anaconda3/envs/tf_1_15_0_new/lib/python3.7/site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /Users/jihyunkang/anaconda3/envs/tf_1_15_0_new/lib/python3.7/site-packages (from tensorflow) (0.33.6)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /Users/jihyunkang/anaconda3/envs/tf_1_15_0_new/lib/python3.7/site-packages (from tensorflow) (1.17.4)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /Users/jihyunkang/anaconda3/envs/tf_1_15_0_new/lib/python3.7/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /Users/jihyunkang/anaconda3/envs/tf_1_15_0_new/lib/python3.7/site-packages (from tensorflow) (1.13.0)\n",
      "Collecting tensorboard<2.1.0,>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 49.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: gast==0.2.2 in /Users/jihyunkang/anaconda3/envs/tf_1_15_0_new/lib/python3.7/site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /Users/jihyunkang/anaconda3/envs/tf_1_15_0_new/lib/python3.7/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /Users/jihyunkang/anaconda3/envs/tf_1_15_0_new/lib/python3.7/site-packages (from tensorflow) (0.1.8)\n",
      "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 41.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /Users/jihyunkang/anaconda3/envs/tf_1_15_0_new/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /Users/jihyunkang/anaconda3/envs/tf_1_15_0_new/lib/python3.7/site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /Users/jihyunkang/anaconda3/envs/tf_1_15_0_new/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /Users/jihyunkang/anaconda3/envs/tf_1_15_0_new/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow) (42.0.1.post20191125)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /Users/jihyunkang/anaconda3/envs/tf_1_15_0_new/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /Users/jihyunkang/anaconda3/envs/tf_1_15_0_new/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/cb/786dc53d93494784935a62947643b48250b84a882474e714f9af5e1a1928/google_auth-1.7.1-py2.py3-none-any.whl (74kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 14.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests<3,>=2.21.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 11.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/50/bb4cefca37da63a0c52218ba2cb1b1c36110d84dcbae8aa48cd67c5e95c2/pyasn1_modules-0.2.7-py2.py3-none-any.whl (131kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 34.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<3.2,>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/2f/a6/30b0a0bef12283e83e58c1d6e7b5aabc7acfc4110df81a4471655d33e704/cachetools-3.1.1-py2.py3-none-any.whl\n",
      "Collecting rsa<4.1,>=3.1.4\n",
      "  Downloading https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/40/a9837291310ee1ccc242ceb6ebfd9eb21539649f193a7c8c86ba15b98539/urllib3-1.25.7-py2.py3-none-any.whl (125kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 26.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting chardet<3.1.0,>=3.0.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 13.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<2.9,>=2.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 16.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/jihyunkang/anaconda3/envs/tf_1_15_0_new/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2019.9.11)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 15.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 26.7MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pyasn1, pyasn1-modules, cachetools, rsa, google-auth, urllib3, chardet, idna, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, tensorflow-estimator, tensorflow\n",
      "  Found existing installation: tensorboard 1.15.0\n",
      "    Uninstalling tensorboard-1.15.0:\n",
      "      Successfully uninstalled tensorboard-1.15.0\n",
      "  Found existing installation: tensorflow-estimator 1.15.1\n",
      "    Uninstalling tensorflow-estimator-1.15.1:\n",
      "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
      "  Found existing installation: tensorflow 1.15.0\n",
      "    Uninstalling tensorflow-1.15.0:\n",
      "      Successfully uninstalled tensorflow-1.15.0\n",
      "Successfully installed cachetools-3.1.1 chardet-3.0.4 google-auth-1.7.1 google-auth-oauthlib-0.4.1 idna-2.8 oauthlib-3.1.0 pyasn1-0.4.8 pyasn1-modules-0.2.7 requests-2.22.0 requests-oauthlib-1.3.0 rsa-4.0 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1 urllib3-1.25.7\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneration:\n",
    "\n",
    "    # target_position = 0 (첫번째열이 정답데이터), target_position=-1 (마지막열이 정답데이터)\n",
    "    def __init__(self, name, file_path, seperation_rate, target_position=-1):\n",
    "\n",
    "        self.name = name\n",
    "\n",
    "        self.file_path = file_path\n",
    "\n",
    "        self.seperation_rate = seperation_rate\n",
    "\n",
    "        if (target_position == -1  or  target_position == 0):\n",
    "            self.target_position = target_position\n",
    "\n",
    "        else:\n",
    "            err_str = 'target_position must be -1 or 0'\n",
    "            raise Exception(err_str)\n",
    "\n",
    "\n",
    "    # print data target distribution\n",
    "    # str_of_kind : 'original data' or  'training data'  or  'test data'\n",
    "    def print_target_distribution(self, data, str_of_kind='original data'):\n",
    "\n",
    "        print('=======================================================================================================')\n",
    "\n",
    "        target_data = data[ :, self.target_position ]\n",
    "\n",
    "        # numpy.unique() 사용하여 loaded data target 분포 확인\n",
    "        unique, counts = np.unique(target_data, return_counts=True)\n",
    "\n",
    "        print('[DataGeneration]  ', str_of_kind, ' target value = ', dict(zip(unique, counts)).items())\n",
    "\n",
    "        num_zeros = dict(zip(unique, counts))[0.0]  # key 0.0 에 대한 value 값 count 리턴\n",
    "        num_ones = dict(zip(unique, counts))[1.0]  # key 1.0 에 대한 value 값 count 리턴\n",
    "\n",
    "        print('[DataGeneration]  ', str_of_kind, ' zeros numbers = ', num_zeros, ', ratio = ', 100 * num_zeros / (data.shape[0]), ' %')\n",
    "        print('[DataGeneration]  ', str_of_kind, ' ones numbers = ', num_ones, ', ratio = ', 100 * num_ones / (data.shape[0]), '%')\n",
    "\n",
    "        print('=======================================================================================================')\n",
    "\n",
    "\n",
    "    # shuffle 기능을 이용하여 training_data / test_data 생성\n",
    "    def generate(self):\n",
    "\n",
    "        # 데이터 불러오기, 파일이 없는 경우 exception 발생\n",
    "\n",
    "        try:\n",
    "            loaded_data = np.loadtxt(self.file_path, delimiter=',', dtype=np.float32)\n",
    "\n",
    "        except Exception as err:\n",
    "            print('[DataGeneration::generate()]  ', str(err))\n",
    "            raise Exception(str(err))\n",
    "\n",
    "        print(\"[DataGeneration]  loaded_data.shape = \", loaded_data.shape)\n",
    "\n",
    "        # print the target distribution of original data\n",
    "\n",
    "        self.print_target_distribution(loaded_data, 'original data')\n",
    "\n",
    "\n",
    "        # random.shuffle() 이용한 데이터 인덱스 분리 및 트레이닝/테스트 데이터 생성\n",
    "\n",
    "        # 임시 저장 리스트\n",
    "        training_data_list = []\n",
    "        test_data_list = []\n",
    "\n",
    "        # 분리비율에 맞게 테스트데이터로 분리\n",
    "        total_data_num = len(loaded_data)\n",
    "        test_data_num = int(len(loaded_data) * self.seperation_rate)\n",
    "\n",
    "        #print(\"[DataGeneration]  total_data_num = \", total_data_num, \", test_data_num = \", test_data_num)\n",
    "\n",
    "        # 전체 데이터 인덱스를 가지고 있는 리스트 생성\n",
    "        total_data_index_list = [ index for index in range(total_data_num) ]\n",
    "\n",
    "        # random.shuffle 을 이용하여 인덱스 리스트 생성\n",
    "        random.shuffle(total_data_index_list)  # 전체 인덱스가 랜덤하게 섞여진 리스트로 변형된다\n",
    "\n",
    "        # test data 를 위한 인덱스는 total_data_index_list 로뷰터 앞에서 분리비율(seperation_rate)의 데이터 인덱스\n",
    "        test_data_index_list = total_data_index_list[ 0:test_data_num ]\n",
    "\n",
    "        #print(\"[DataGeneration]  length of test_data_index_list = \", len(test_data_index_list))\n",
    "\n",
    "        # training data 를 위한 인덱스는 total_data_index_list 에서 test data 인덱스를 제외한 나머지 부분\n",
    "        training_data_index_list = total_data_index_list[ test_data_num: ]\n",
    "\n",
    "        #print(\"[DataGeneration]  length of training_data_index_list = \", len(training_data_index_list))\n",
    "\n",
    "        # training data 구성\n",
    "        for training_data_index in training_data_index_list:\n",
    "\n",
    "            training_data_list.append(loaded_data[training_data_index])\n",
    "\n",
    "        # test data 구성\n",
    "        for test_data_index in test_data_index_list:\n",
    "\n",
    "            test_data_list.append(loaded_data[test_data_index])\n",
    "\n",
    "        # generate training data from training_data_list using np.arrya(...)\n",
    "        training_data = np.array(training_data_list)\n",
    "\n",
    "        # generate test data from test_data_list using np.arrya(...)\n",
    "        test_data = np.array(test_data_list)\n",
    "\n",
    "        # verification shape\n",
    "        #print(\"[DataGeneration]  training_data.shape = \", training_data.shape)\n",
    "        #print(\"[DataGeneration]  test_data.shape = \", test_data.shape)\n",
    "\n",
    "        # print target distribution of generated data\n",
    "\n",
    "        self.print_target_distribution(training_data, 'training data')\n",
    "\n",
    "        self.print_target_distribution(test_data, 'test data')\n",
    "\n",
    "\n",
    "        # save training & test data (.csv)\n",
    "        training_data_save_path = './' + self.name + '_training_data.csv'\n",
    "        test_data_save_path = './' + self.name + '_test_data.csv'\n",
    "\n",
    "        # 저장공간이 없거나 파일 write 실패시 exception 발생\n",
    "        try:\n",
    "            np.savetxt(training_data_save_path, training_data, delimiter=',')\n",
    "            np.savetxt(test_data_save_path, test_data, delimiter=',')\n",
    "\n",
    "        except Exception as err:\n",
    "            print('[DataGeneration::generate()]  ', str(err))\n",
    "            raise Exception(str(err))\n",
    "\n",
    "        return training_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataGeneration]  loaded_data.shape =  (759, 9)\n",
      "=======================================================================================================\n",
      "[DataGeneration]   original data  target value =  dict_items([(0.0, 263), (1.0, 496)])\n",
      "[DataGeneration]   original data  zeros numbers =  263 , ratio =  34.65085638998683  %\n",
      "[DataGeneration]   original data  ones numbers =  496 , ratio =  65.34914361001317 %\n",
      "=======================================================================================================\n",
      "=======================================================================================================\n",
      "[DataGeneration]   training data  target value =  dict_items([(0.0, 157), (1.0, 299)])\n",
      "[DataGeneration]   training data  zeros numbers =  157 , ratio =  34.42982456140351  %\n",
      "[DataGeneration]   training data  ones numbers =  299 , ratio =  65.5701754385965 %\n",
      "=======================================================================================================\n",
      "=======================================================================================================\n",
      "[DataGeneration]   test data  target value =  dict_items([(0.0, 106), (1.0, 197)])\n",
      "[DataGeneration]   test data  zeros numbers =  106 , ratio =  34.98349834983498  %\n",
      "[DataGeneration]   test data  ones numbers =  197 , ratio =  65.01650165016501 %\n",
      "=======================================================================================================\n",
      "training_data.shape =  (456, 9)\n",
      "test_data.shape =  (303, 9)\n"
     ]
    }
   ],
   "source": [
    "# DataGeneration 객체 생성\n",
    "seperation_rate = 0.4\n",
    "data_obj = DataGeneration('Diabetes', './(191117)diabetes.csv', seperation_rate)\n",
    "\n",
    "# training_data, test_data 생성\n",
    "(training_data, test_data) = data_obj.generate()\n",
    "\n",
    "print(\"training_data.shape = \", training_data.shape)\n",
    "print(\"test_data.shape = \", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_x_data.shape =  (456, 8)\n",
      "training_t_data.shape =  (456, 1)\n",
      "test_x_data.shape =  (303, 8)\n",
      "test_x_data.shape =  (303, 8)\n"
     ]
    }
   ],
   "source": [
    "training_x_data = training_data[ :, 0:-1]\n",
    "training_t_data = training_data[ :, [-1]]\n",
    "\n",
    "print(\"training_x_data.shape = \", training_x_data.shape)\n",
    "print(\"training_t_data.shape = \", training_t_data.shape)\n",
    "\n",
    "test_x_data = test_data[ :, 0:-1]\n",
    "test_t_data = test_data[ :, [-1]]\n",
    "\n",
    "print(\"test_x_data.shape = \", test_x_data.shape)\n",
    "print(\"test_x_data.shape = \", test_x_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow 2.0 에서는 즉시실행 모드 때문에 placeholder 사용시 런타임 에러 발생\n",
    "### error message : RuntimeError: tf.placeholder() is not compatible with eager execution.\n",
    "\n",
    "### 해결책 : 다음 문장을 placeholder 실행 전에 execute한다.\n",
    "#### statement : tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RuntimeError : tf.placeholder() is not compatible with eager execution. 해결\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jihyunkang/anaconda3/envs/tf_1_15_0_new/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "X = tf.compat.v1.placeholder(tf.float32, [None, 8])  # 8개 입력노드\n",
    "T = tf.compat.v1.placeholder(tf.float32, [None, 1])  # 1개 정답노드\n",
    "\n",
    "W = tf.Variable(tf.random.normal([8, 1]))  # 8X1 가중치노드\n",
    "b = tf.Variable(tf.random.normal([1]))     # 1개 바이어스 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.matmul(X, W) + b  # 선형회귀 값 z\n",
    "\n",
    "y = tf.sigmoid(z)    # 시그모이드로 계산 값\n",
    "\n",
    "# 손실함수는 Cross-Entropy\n",
    "loss = -tf.reduce_mean( input_tensor=T*tf.math.log(y) + (1-T)*tf.math.log(1-y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01    # 학습율\n",
    "\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확성 검사, True if y > 0.5 else False\n",
    "\n",
    "predicted = tf.cast(y > 0.5, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# predicted 와 T 같으면 True 를 리턴하므로 cast 에 의해서 1로 강제 변환,\n",
    "accuracy = tf.reduce_mean(input_tensor=tf.cast(tf.equal(predicted, T), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  0 , loss_val =  0.9230915\n",
      "step =  500 , loss_val =  0.7347302\n",
      "step =  1000 , loss_val =  0.6825774\n",
      "step =  1500 , loss_val =  0.6418764\n",
      "step =  2000 , loss_val =  0.6093674\n",
      "step =  2500 , loss_val =  0.58329374\n",
      "step =  3000 , loss_val =  0.5622561\n",
      "step =  3500 , loss_val =  0.54516196\n",
      "step =  4000 , loss_val =  0.5311673\n",
      "step =  4500 , loss_val =  0.5196229\n",
      "step =  5000 , loss_val =  0.5100283\n",
      "step =  5500 , loss_val =  0.50199604\n",
      "step =  6000 , loss_val =  0.49522516\n",
      "step =  6500 , loss_val =  0.48948014\n",
      "step =  7000 , loss_val =  0.4845752\n",
      "step =  7500 , loss_val =  0.480363\n",
      "step =  8000 , loss_val =  0.47672573\n",
      "step =  8500 , loss_val =  0.47356862\n",
      "step =  9000 , loss_val =  0.4708151\n",
      "step =  9500 , loss_val =  0.4684023\n",
      "step =  10000 , loss_val =  0.466279\n",
      "step =  10500 , loss_val =  0.4644027\n",
      "step =  11000 , loss_val =  0.46273842\n",
      "step =  11500 , loss_val =  0.46125653\n",
      "step =  12000 , loss_val =  0.4599326\n",
      "step =  12500 , loss_val =  0.45874575\n",
      "step =  13000 , loss_val =  0.45767838\n",
      "step =  13500 , loss_val =  0.45671564\n",
      "step =  14000 , loss_val =  0.45584464\n",
      "step =  14500 , loss_val =  0.45505464\n",
      "step =  15000 , loss_val =  0.45433608\n",
      "step =  15500 , loss_val =  0.4536808\n",
      "step =  16000 , loss_val =  0.45308185\n",
      "step =  16500 , loss_val =  0.45253307\n",
      "step =  17000 , loss_val =  0.4520291\n",
      "step =  17500 , loss_val =  0.45156527\n",
      "step =  18000 , loss_val =  0.45113754\n",
      "step =  18500 , loss_val =  0.45074227\n",
      "step =  19000 , loss_val =  0.45037633\n",
      "step =  19500 , loss_val =  0.45003682\n",
      "step =  20000 , loss_val =  0.44972146\n",
      "\n",
      "y_val.shape =  (303, 1) , predicted_val =  (303, 1)\n",
      "\n",
      "Accuracy =  0.7326733\n"
     ]
    }
   ],
   "source": [
    "with  tf.compat.v1.Session()  as sess:\n",
    "\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())  # 변수 노드(tf.Variable) 초기화\n",
    "\n",
    "    for step in range(20001):\n",
    "\n",
    "        loss_val, _ = sess.run([loss, train], feed_dict={X: training_x_data, T: training_t_data})\n",
    "\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            print(\"step = \", step, \", loss_val = \", loss_val)\n",
    "\n",
    "    # Accuracy 확인\n",
    "    y_val, predicted_val, accuracy_val = sess.run([y, predicted, accuracy], feed_dict={X: test_x_data, T: test_t_data})\n",
    "\n",
    "\n",
    "    print(\"\\ny_val.shape = \", y_val.shape, \", predicted_val = \", predicted_val.shape)\n",
    "    print(\"\\nAccuracy = \", accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
