{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "hii0aBNc3dBu",
    "outputId": "de6958e6-58dd-44cc-aab0-8f631490baef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-90b8c2e86d09>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "\n",
      "train.num =  55000 , test.num =  10000 , validation.num =  5000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "print(\"\")\n",
    "print(\"train.num = \", mnist.train.num_examples, \n",
    "      \", test.num = \", mnist.test.num_examples, \n",
    "      \", validation.num = \", mnist.validation.num_examples) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8bAVGMSD3dB9"
   },
   "source": [
    "#### shape 및 type(mnist) 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "colab_type": "code",
    "id": "GV7tnrcl3dB-",
    "outputId": "6c0d8236-5b34-4ebe-f06e-ac8802318de4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(mnist) =  <class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'> , type(mnist.train.images) =  <class 'numpy.ndarray'> , type(mnist.train.labels) =  <class 'numpy.ndarray'>\n",
      "\n",
      "train image shape =  (55000, 784)\n",
      "train label shape =  (55000, 10)\n",
      "test image shape =  (10000, 784)\n",
      "test label shape =  (10000, 10)\n",
      "\n",
      "train image shape =  (55000, 784)\n",
      "test image shape =  (10000, 784)\n",
      "validation image shape =  (5000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(\"type(mnist) = \", type(mnist), \n",
    "      \", type(mnist.train.images) = \", type(mnist.train.images), \n",
    "      \", type(mnist.train.labels) = \", type(mnist.train.labels))\n",
    "\n",
    "print(\"\\ntrain image shape = \", np.shape(mnist.train.images))\n",
    "print(\"train label shape = \", np.shape(mnist.train.labels))\n",
    "print(\"test image shape = \", np.shape(mnist.test.images))\n",
    "print(\"test label shape = \", np.shape(mnist.test.labels))\n",
    "\n",
    "print(\"\\ntrain image shape = \", mnist.train.images.shape)\n",
    "print(\"test image shape = \", mnist.test.images.shape)\n",
    "print(\"validation image shape = \", mnist.validation.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ohhMTGX3dCC"
   },
   "source": [
    "#### train data 정규화 및 label 의 one-hot encoding 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Q9U5KMpD3dCE",
    "outputId": "73f2cb1e-f9b0-4636-b7e6-a795b8a13edd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of mnist.train.images =  55000\n",
      "\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.3803922  0.37647063 0.3019608\n",
      " 0.46274513 0.2392157  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.3529412\n",
      " 0.5411765  0.9215687  0.9215687  0.9215687  0.9215687  0.9215687\n",
      " 0.9215687  0.9843138  0.9843138  0.9725491  0.9960785  0.9607844\n",
      " 0.9215687  0.74509805 0.08235294 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.54901963 0.9843138  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.7411765  0.09019608 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.8862746  0.9960785  0.81568635 0.7803922  0.7803922  0.7803922\n",
      " 0.7803922  0.54509807 0.2392157  0.2392157  0.2392157  0.2392157\n",
      " 0.2392157  0.5019608  0.8705883  0.9960785  0.9960785  0.7411765\n",
      " 0.08235294 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.14901961 0.32156864\n",
      " 0.0509804  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.13333334 0.8352942  0.9960785  0.9960785  0.45098042 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.32941177\n",
      " 0.9960785  0.9960785  0.9176471  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.32941177 0.9960785  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4156863  0.6156863  0.9960785  0.9960785  0.95294124 0.20000002\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.09803922\n",
      " 0.45882356 0.8941177  0.8941177  0.8941177  0.9921569  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.94117653 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.26666668 0.4666667  0.86274517 0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.5568628  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.14509805 0.73333335 0.9921569\n",
      " 0.9960785  0.9960785  0.9960785  0.8745099  0.8078432  0.8078432\n",
      " 0.29411766 0.26666668 0.8431373  0.9960785  0.9960785  0.45882356\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4431373  0.8588236  0.9960785  0.9490197  0.89019614 0.45098042\n",
      " 0.34901962 0.12156864 0.         0.         0.         0.\n",
      " 0.7843138  0.9960785  0.9450981  0.16078432 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.6627451  0.9960785\n",
      " 0.6901961  0.24313727 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.18823531 0.9058824  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.07058824 0.48627454 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.32941177 0.9960785  0.9960785  0.6509804  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.54509807\n",
      " 0.9960785  0.9333334  0.22352943 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.8235295  0.9803922  0.9960785  0.65882355\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.9490197  0.9960785  0.93725497 0.22352943 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.34901962 0.9843138  0.9450981\n",
      " 0.3372549  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01960784 0.8078432  0.96470594 0.6156863  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01568628 0.45882356\n",
      " 0.27058825 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# 정규화 확인\n",
    "print(\"length of mnist.train.images = \", len(mnist.train.images))\n",
    "\n",
    "for index in range(len(mnist.train.images)):\n",
    "    \n",
    "    min_val = np.min(mnist.train.images[index])\n",
    "    max_val = np.max(mnist.train.images[index])\n",
    "    \n",
    "    if min_val < 0.0:\n",
    "        print(\"min value is \", min_val, \", index = \", index)\n",
    "        break\n",
    "    \n",
    "    if max_val > 1.0:\n",
    "        print(\"max value is \", max_val, \", index = \", index)\n",
    "        break\n",
    "    \n",
    "print(\"\")\n",
    "print(mnist.train.images[0])  # 정규화 확인을 위한 테스트 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "_CJZqaWY3dCJ",
    "outputId": "f7622ded-8c54-486e-c1d5-5d11a634372c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of mnist.train.images =  55000\n",
      "\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding 확인\n",
    "print(\"length of mnist.train.images = \", len(mnist.train.labels))\n",
    "\n",
    "for index in range(len(mnist.train.labels)):\n",
    "    \n",
    "    min_val = np.min(mnist.train.labels[index])\n",
    "    max_val = np.max(mnist.train.labels[index])\n",
    "    \n",
    "    if min_val < 0.0:\n",
    "        print(\"min value is \", min_val, \", index = \", index)\n",
    "        break\n",
    "    \n",
    "    if max_val > 1.0:\n",
    "        print(\"max value is \", max_val, \", index = \", index)\n",
    "        break\n",
    "    \n",
    "print(\"\")\n",
    "print(mnist.train.labels[0])  # one-hot encoding 확인을 위한 테스트 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "modsPHJk3dCN"
   },
   "source": [
    "#### Hyper-Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-A1Zg62n3dCO"
   },
   "outputs": [],
   "source": [
    "# 입력노드, 은닉노드, 출력노드, 학습율, 반복횟수, 배치 개수 등 설정\n",
    "learning_rate = 0.1  # 학습율\n",
    "epochs = 50            # 반복횟수\n",
    "batch_size = 100      # 한번에 입력으로 주어지는 MNIST 개수\n",
    "\n",
    "input_nodes = 784     # 입력노드 개수\n",
    "hidden_nodes = 100    # 은닉노드 개수\n",
    "output_nodes = 10     # 출력노드 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AiUHKuBe3dCW"
   },
   "source": [
    "#### 입력과 출력을 위한 플레이스홀더 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lWEwL-823dCX"
   },
   "outputs": [],
   "source": [
    "# 입력과 출력을 위한 플레이스홀더 정의\n",
    "X = tf.placeholder(tf.float32, [None, input_nodes])  \n",
    "T = tf.placeholder(tf.float32, [None, output_nodes])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_4h7965P3dCc"
   },
   "source": [
    "#### 가중치, 바이어스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3h1WGBDF3dCd"
   },
   "outputs": [],
   "source": [
    "W2 = tf.Variable(tf.random_normal([input_nodes, hidden_nodes]))  # 은닉층 가중치 노드\n",
    "b2 = tf.Variable(tf.random_normal([hidden_nodes]))               # 은닉층 바이어스 노드\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([hidden_nodes, output_nodes])) # 출력층 가중치 노드\n",
    "b3 = tf.Variable(tf.random_normal([output_nodes]))               # 출력층 바이어스 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C85SQmbt3dCh"
   },
   "outputs": [],
   "source": [
    "Z2 = tf.matmul(X, W2) + b2    # 선형회귀 선형회귀 값 Z2\n",
    "A2 = tf.nn.relu(Z2)           # 은닉층 출력 값 A2, sigmoid 대신 relu 사용\n",
    "\n",
    "# 출력층 선형회귀  값 Z3, 즉 softmax 에 들어가는 입력 값\n",
    "Z3 = logits = tf.matmul(A2, W3) + b3   \n",
    "\n",
    "y = A3 = tf.nn.softmax(Z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SA4oMwL73dCl"
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z3, labels=T) )\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "train = optimizer.minimize(loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AAtYhqFg3dCp"
   },
   "outputs": [],
   "source": [
    "# batch_size X 10 데이터에 대해 argmax를 통해 행단위로 비교함\n",
    "predicted_val = tf.equal( tf.argmax(A3, 1), tf.argmax(T, 1) )\n",
    "\n",
    "# batch_size X 10 의 True, False 를 1 또는 0 으로 변환\n",
    "accuracy = tf.reduce_mean(tf.cast(predicted_val, dtype=tf.float32))\n",
    "\n",
    "# index list 출력\n",
    "accuracy_index = tf.cast(predicted_val, dtype=tf.float32)\n",
    "\n",
    "# 예측값 처리\n",
    "predicted_list = tf.argmax(A3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "yTd24gV23dCt",
    "outputId": "c7b65d94-7ce8-44e0-9e39-714ca71abfa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(predicted_val) =  <class 'tensorflow.python.framework.ops.Tensor'> , type(accuracy) =  <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "type(accuracy_index) = <class 'tensorflow.python.framework.ops.Tensor'> , type(predicted_list) =  <class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Type Check\n",
    "print('type(predicted_val) = ', type(predicted_val),  ', type(accuracy) = ', type(accuracy))\n",
    "print('type(accuracy_index) =', type(accuracy_index), ', type(predicted_list) = ', type(predicted_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0msF9q3x3dCx",
    "outputId": "b226b632-dad1-4bbe-e463-c95748efbfc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  0 , step =  0 , loss_val =  104.42905\n",
      "epochs =  0 , step =  100 , loss_val =  5.4493346\n",
      "epochs =  0 , step =  200 , loss_val =  2.5294538\n",
      "epochs =  0 , step =  300 , loss_val =  1.6645944\n",
      "epochs =  0 , step =  400 , loss_val =  2.466918\n",
      "epochs =  0 , step =  500 , loss_val =  0.79886657\n",
      "epochs =  1 , step =  0 , loss_val =  1.1780095\n",
      "epochs =  1 , step =  100 , loss_val =  0.49604622\n",
      "epochs =  1 , step =  200 , loss_val =  0.8166074\n",
      "epochs =  1 , step =  300 , loss_val =  1.3740922\n",
      "epochs =  1 , step =  400 , loss_val =  1.1018409\n",
      "epochs =  1 , step =  500 , loss_val =  0.88690645\n",
      "epochs =  2 , step =  0 , loss_val =  0.81971145\n",
      "epochs =  2 , step =  100 , loss_val =  0.74649274\n",
      "epochs =  2 , step =  200 , loss_val =  0.7310297\n",
      "epochs =  2 , step =  300 , loss_val =  0.320104\n",
      "epochs =  2 , step =  400 , loss_val =  0.44611374\n",
      "epochs =  2 , step =  500 , loss_val =  0.6496271\n",
      "epochs =  3 , step =  0 , loss_val =  0.46580973\n",
      "epochs =  3 , step =  100 , loss_val =  0.3792227\n",
      "epochs =  3 , step =  200 , loss_val =  1.1741802\n",
      "epochs =  3 , step =  300 , loss_val =  0.35309848\n",
      "epochs =  3 , step =  400 , loss_val =  0.4411272\n",
      "epochs =  3 , step =  500 , loss_val =  0.46183214\n",
      "epochs =  4 , step =  0 , loss_val =  0.3636661\n",
      "epochs =  4 , step =  100 , loss_val =  0.36950874\n",
      "epochs =  4 , step =  200 , loss_val =  0.6582124\n",
      "epochs =  4 , step =  300 , loss_val =  0.41675386\n",
      "epochs =  4 , step =  400 , loss_val =  0.2316559\n",
      "epochs =  4 , step =  500 , loss_val =  0.38319886\n",
      "epochs =  5 , step =  0 , loss_val =  0.47938633\n",
      "epochs =  5 , step =  100 , loss_val =  0.49155602\n",
      "epochs =  5 , step =  200 , loss_val =  0.38632584\n",
      "epochs =  5 , step =  300 , loss_val =  0.46011943\n",
      "epochs =  5 , step =  400 , loss_val =  0.2618798\n",
      "epochs =  5 , step =  500 , loss_val =  0.4250506\n",
      "epochs =  6 , step =  0 , loss_val =  0.25157142\n",
      "epochs =  6 , step =  100 , loss_val =  0.33226314\n",
      "epochs =  6 , step =  200 , loss_val =  0.28466716\n",
      "epochs =  6 , step =  300 , loss_val =  0.48274338\n",
      "epochs =  6 , step =  400 , loss_val =  0.48611596\n",
      "epochs =  6 , step =  500 , loss_val =  0.18138267\n",
      "epochs =  7 , step =  0 , loss_val =  0.526078\n",
      "epochs =  7 , step =  100 , loss_val =  0.3713571\n",
      "epochs =  7 , step =  200 , loss_val =  0.35872144\n",
      "epochs =  7 , step =  300 , loss_val =  0.32018167\n",
      "epochs =  7 , step =  400 , loss_val =  0.4526458\n",
      "epochs =  7 , step =  500 , loss_val =  0.686716\n",
      "epochs =  8 , step =  0 , loss_val =  0.26006103\n",
      "epochs =  8 , step =  100 , loss_val =  0.20266815\n",
      "epochs =  8 , step =  200 , loss_val =  0.67326605\n",
      "epochs =  8 , step =  300 , loss_val =  0.18188845\n",
      "epochs =  8 , step =  400 , loss_val =  0.37038872\n",
      "epochs =  8 , step =  500 , loss_val =  0.5648355\n",
      "epochs =  9 , step =  0 , loss_val =  0.1372307\n",
      "epochs =  9 , step =  100 , loss_val =  0.35309193\n",
      "epochs =  9 , step =  200 , loss_val =  0.34860635\n",
      "epochs =  9 , step =  300 , loss_val =  0.22743708\n",
      "epochs =  9 , step =  400 , loss_val =  0.22372258\n",
      "epochs =  9 , step =  500 , loss_val =  0.47685504\n",
      "epochs =  10 , step =  0 , loss_val =  0.20115575\n",
      "epochs =  10 , step =  100 , loss_val =  0.2074912\n",
      "epochs =  10 , step =  200 , loss_val =  0.11634885\n",
      "epochs =  10 , step =  300 , loss_val =  0.5992077\n",
      "epochs =  10 , step =  400 , loss_val =  0.63934034\n",
      "epochs =  10 , step =  500 , loss_val =  0.23805296\n",
      "epochs =  11 , step =  0 , loss_val =  0.2458618\n",
      "epochs =  11 , step =  100 , loss_val =  0.30709544\n",
      "epochs =  11 , step =  200 , loss_val =  0.2890628\n",
      "epochs =  11 , step =  300 , loss_val =  0.15991288\n",
      "epochs =  11 , step =  400 , loss_val =  0.29249662\n",
      "epochs =  11 , step =  500 , loss_val =  0.58454967\n",
      "epochs =  12 , step =  0 , loss_val =  0.19302227\n",
      "epochs =  12 , step =  100 , loss_val =  0.2034776\n",
      "epochs =  12 , step =  200 , loss_val =  0.24672538\n",
      "epochs =  12 , step =  300 , loss_val =  0.24486698\n",
      "epochs =  12 , step =  400 , loss_val =  0.5149044\n",
      "epochs =  12 , step =  500 , loss_val =  0.28407586\n",
      "epochs =  13 , step =  0 , loss_val =  0.170885\n",
      "epochs =  13 , step =  100 , loss_val =  0.1439799\n",
      "epochs =  13 , step =  200 , loss_val =  0.09746667\n",
      "epochs =  13 , step =  300 , loss_val =  0.20851058\n",
      "epochs =  13 , step =  400 , loss_val =  0.16358963\n",
      "epochs =  13 , step =  500 , loss_val =  0.22899653\n",
      "epochs =  14 , step =  0 , loss_val =  0.31414378\n",
      "epochs =  14 , step =  100 , loss_val =  0.284528\n",
      "epochs =  14 , step =  200 , loss_val =  0.24643308\n",
      "epochs =  14 , step =  300 , loss_val =  0.26206994\n",
      "epochs =  14 , step =  400 , loss_val =  0.10502388\n",
      "epochs =  14 , step =  500 , loss_val =  0.29109955\n",
      "epochs =  15 , step =  0 , loss_val =  0.36169383\n",
      "epochs =  15 , step =  100 , loss_val =  0.30940998\n",
      "epochs =  15 , step =  200 , loss_val =  0.22810185\n",
      "epochs =  15 , step =  300 , loss_val =  0.1478838\n",
      "epochs =  15 , step =  400 , loss_val =  0.20127127\n",
      "epochs =  15 , step =  500 , loss_val =  0.14416665\n",
      "epochs =  16 , step =  0 , loss_val =  0.32535666\n",
      "epochs =  16 , step =  100 , loss_val =  0.14661223\n",
      "epochs =  16 , step =  200 , loss_val =  0.36652726\n",
      "epochs =  16 , step =  300 , loss_val =  0.22242603\n",
      "epochs =  16 , step =  400 , loss_val =  0.22629379\n",
      "epochs =  16 , step =  500 , loss_val =  0.2013308\n",
      "epochs =  17 , step =  0 , loss_val =  0.3046385\n",
      "epochs =  17 , step =  100 , loss_val =  0.18630882\n",
      "epochs =  17 , step =  200 , loss_val =  0.2141131\n",
      "epochs =  17 , step =  300 , loss_val =  0.2902231\n",
      "epochs =  17 , step =  400 , loss_val =  0.1862703\n",
      "epochs =  17 , step =  500 , loss_val =  0.28574947\n",
      "epochs =  18 , step =  0 , loss_val =  0.14749846\n",
      "epochs =  18 , step =  100 , loss_val =  0.34645662\n",
      "epochs =  18 , step =  200 , loss_val =  0.25065976\n",
      "epochs =  18 , step =  300 , loss_val =  0.20510596\n",
      "epochs =  18 , step =  400 , loss_val =  0.19640602\n",
      "epochs =  18 , step =  500 , loss_val =  0.15251574\n",
      "epochs =  19 , step =  0 , loss_val =  0.1579248\n",
      "epochs =  19 , step =  100 , loss_val =  0.15085524\n",
      "epochs =  19 , step =  200 , loss_val =  0.25259432\n",
      "epochs =  19 , step =  300 , loss_val =  0.09215686\n",
      "epochs =  19 , step =  400 , loss_val =  0.12689619\n",
      "epochs =  19 , step =  500 , loss_val =  0.3139897\n",
      "epochs =  20 , step =  0 , loss_val =  0.22975266\n",
      "epochs =  20 , step =  100 , loss_val =  0.11219906\n",
      "epochs =  20 , step =  200 , loss_val =  0.34473282\n",
      "epochs =  20 , step =  300 , loss_val =  0.15930812\n",
      "epochs =  20 , step =  400 , loss_val =  0.1914899\n",
      "epochs =  20 , step =  500 , loss_val =  0.2334904\n",
      "epochs =  21 , step =  0 , loss_val =  0.17707495\n",
      "epochs =  21 , step =  100 , loss_val =  0.4049808\n",
      "epochs =  21 , step =  200 , loss_val =  0.11868062\n",
      "epochs =  21 , step =  300 , loss_val =  0.15292545\n",
      "epochs =  21 , step =  400 , loss_val =  0.27874187\n",
      "epochs =  21 , step =  500 , loss_val =  0.27076253\n",
      "epochs =  22 , step =  0 , loss_val =  0.18171972\n",
      "epochs =  22 , step =  100 , loss_val =  0.21114136\n",
      "epochs =  22 , step =  200 , loss_val =  0.2017948\n",
      "epochs =  22 , step =  300 , loss_val =  0.33574414\n",
      "epochs =  22 , step =  400 , loss_val =  0.07250394\n",
      "epochs =  22 , step =  500 , loss_val =  0.16778903\n",
      "epochs =  23 , step =  0 , loss_val =  0.39924073\n",
      "epochs =  23 , step =  100 , loss_val =  0.3167764\n",
      "epochs =  23 , step =  200 , loss_val =  0.1659866\n",
      "epochs =  23 , step =  300 , loss_val =  0.12626706\n",
      "epochs =  23 , step =  400 , loss_val =  0.14985943\n",
      "epochs =  23 , step =  500 , loss_val =  0.09724113\n",
      "epochs =  24 , step =  0 , loss_val =  0.32417116\n",
      "epochs =  24 , step =  100 , loss_val =  0.34179732\n",
      "epochs =  24 , step =  200 , loss_val =  0.13710481\n",
      "epochs =  24 , step =  300 , loss_val =  0.1353775\n",
      "epochs =  24 , step =  400 , loss_val =  0.09638262\n",
      "epochs =  24 , step =  500 , loss_val =  0.16988562\n",
      "epochs =  25 , step =  0 , loss_val =  0.21292526\n",
      "epochs =  25 , step =  100 , loss_val =  0.0858668\n",
      "epochs =  25 , step =  200 , loss_val =  0.13695803\n",
      "epochs =  25 , step =  300 , loss_val =  0.28488803\n",
      "epochs =  25 , step =  400 , loss_val =  0.15750603\n",
      "epochs =  25 , step =  500 , loss_val =  0.18702106\n",
      "epochs =  26 , step =  0 , loss_val =  0.21705109\n",
      "epochs =  26 , step =  100 , loss_val =  0.2112122\n",
      "epochs =  26 , step =  200 , loss_val =  0.13655934\n",
      "epochs =  26 , step =  300 , loss_val =  0.11361588\n",
      "epochs =  26 , step =  400 , loss_val =  0.43413404\n",
      "epochs =  26 , step =  500 , loss_val =  0.11884757\n",
      "epochs =  27 , step =  0 , loss_val =  0.06376566\n",
      "epochs =  27 , step =  100 , loss_val =  0.21006687\n",
      "epochs =  27 , step =  200 , loss_val =  0.23448937\n",
      "epochs =  27 , step =  300 , loss_val =  0.23835105\n",
      "epochs =  27 , step =  400 , loss_val =  0.13490134\n",
      "epochs =  27 , step =  500 , loss_val =  0.16512494\n",
      "epochs =  28 , step =  0 , loss_val =  0.10955452\n",
      "epochs =  28 , step =  100 , loss_val =  0.15471771\n",
      "epochs =  28 , step =  200 , loss_val =  0.09126162\n",
      "epochs =  28 , step =  300 , loss_val =  0.14169174\n",
      "epochs =  28 , step =  400 , loss_val =  0.08577472\n",
      "epochs =  28 , step =  500 , loss_val =  0.1324717\n",
      "epochs =  29 , step =  0 , loss_val =  0.07949381\n",
      "epochs =  29 , step =  100 , loss_val =  0.2319286\n",
      "epochs =  29 , step =  200 , loss_val =  0.22400503\n",
      "epochs =  29 , step =  300 , loss_val =  0.07116709\n",
      "epochs =  29 , step =  400 , loss_val =  0.12627693\n",
      "epochs =  29 , step =  500 , loss_val =  0.22171864\n",
      "epochs =  30 , step =  0 , loss_val =  0.3912721\n",
      "epochs =  30 , step =  100 , loss_val =  0.19282906\n",
      "epochs =  30 , step =  200 , loss_val =  0.2921432\n",
      "epochs =  30 , step =  300 , loss_val =  0.149327\n",
      "epochs =  30 , step =  400 , loss_val =  0.12707551\n",
      "epochs =  30 , step =  500 , loss_val =  0.13438039\n",
      "epochs =  31 , step =  0 , loss_val =  0.13608082\n",
      "epochs =  31 , step =  100 , loss_val =  0.18853822\n",
      "epochs =  31 , step =  200 , loss_val =  0.11039569\n",
      "epochs =  31 , step =  300 , loss_val =  0.28281093\n",
      "epochs =  31 , step =  400 , loss_val =  0.21149752\n",
      "epochs =  31 , step =  500 , loss_val =  0.56280077\n",
      "epochs =  32 , step =  0 , loss_val =  0.056884438\n",
      "epochs =  32 , step =  100 , loss_val =  0.39818943\n",
      "epochs =  32 , step =  200 , loss_val =  0.1974486\n",
      "epochs =  32 , step =  300 , loss_val =  0.19502264\n",
      "epochs =  32 , step =  400 , loss_val =  0.119238585\n",
      "epochs =  32 , step =  500 , loss_val =  0.38181704\n",
      "epochs =  33 , step =  0 , loss_val =  0.031215113\n",
      "epochs =  33 , step =  100 , loss_val =  0.1092208\n",
      "epochs =  33 , step =  200 , loss_val =  0.045157276\n",
      "epochs =  33 , step =  300 , loss_val =  0.22849728\n",
      "epochs =  33 , step =  400 , loss_val =  0.17400719\n",
      "epochs =  33 , step =  500 , loss_val =  0.29733354\n",
      "epochs =  34 , step =  0 , loss_val =  0.13242763\n",
      "epochs =  34 , step =  100 , loss_val =  0.13721636\n",
      "epochs =  34 , step =  200 , loss_val =  0.26793155\n",
      "epochs =  34 , step =  300 , loss_val =  0.097516134\n",
      "epochs =  34 , step =  400 , loss_val =  0.16989487\n",
      "epochs =  34 , step =  500 , loss_val =  0.27335566\n",
      "epochs =  35 , step =  0 , loss_val =  0.15388018\n",
      "epochs =  35 , step =  100 , loss_val =  0.19479279\n",
      "epochs =  35 , step =  200 , loss_val =  0.17859471\n",
      "epochs =  35 , step =  300 , loss_val =  0.08532248\n",
      "epochs =  35 , step =  400 , loss_val =  0.21887743\n",
      "epochs =  35 , step =  500 , loss_val =  0.19368744\n",
      "epochs =  36 , step =  0 , loss_val =  0.10398023\n",
      "epochs =  36 , step =  100 , loss_val =  0.13505666\n",
      "epochs =  36 , step =  200 , loss_val =  0.18164964\n",
      "epochs =  36 , step =  300 , loss_val =  0.12219609\n",
      "epochs =  36 , step =  400 , loss_val =  0.1873159\n",
      "epochs =  36 , step =  500 , loss_val =  0.08194347\n",
      "epochs =  37 , step =  0 , loss_val =  0.3374581\n",
      "epochs =  37 , step =  100 , loss_val =  0.19939305\n",
      "epochs =  37 , step =  200 , loss_val =  0.33569086\n",
      "epochs =  37 , step =  300 , loss_val =  0.21137682\n",
      "epochs =  37 , step =  400 , loss_val =  0.21569988\n",
      "epochs =  37 , step =  500 , loss_val =  0.34546357\n",
      "epochs =  38 , step =  0 , loss_val =  0.21451807\n",
      "epochs =  38 , step =  100 , loss_val =  0.04845902\n",
      "epochs =  38 , step =  200 , loss_val =  0.093056366\n",
      "epochs =  38 , step =  300 , loss_val =  0.08562323\n",
      "epochs =  38 , step =  400 , loss_val =  0.08249653\n",
      "epochs =  38 , step =  500 , loss_val =  0.15578309\n",
      "epochs =  39 , step =  0 , loss_val =  0.15750133\n",
      "epochs =  39 , step =  100 , loss_val =  0.13886335\n",
      "epochs =  39 , step =  200 , loss_val =  0.2718256\n",
      "epochs =  39 , step =  300 , loss_val =  0.17512514\n",
      "epochs =  39 , step =  400 , loss_val =  0.107107684\n",
      "epochs =  39 , step =  500 , loss_val =  0.20112383\n",
      "epochs =  40 , step =  0 , loss_val =  0.08372709\n",
      "epochs =  40 , step =  100 , loss_val =  0.13643116\n",
      "epochs =  40 , step =  200 , loss_val =  0.12799042\n",
      "epochs =  40 , step =  300 , loss_val =  0.08118883\n",
      "epochs =  40 , step =  400 , loss_val =  0.112751156\n",
      "epochs =  40 , step =  500 , loss_val =  0.10905853\n",
      "epochs =  41 , step =  0 , loss_val =  0.24672875\n",
      "epochs =  41 , step =  100 , loss_val =  0.14066166\n",
      "epochs =  41 , step =  200 , loss_val =  0.11304092\n",
      "epochs =  41 , step =  300 , loss_val =  0.107108496\n",
      "epochs =  41 , step =  400 , loss_val =  0.16483717\n",
      "epochs =  41 , step =  500 , loss_val =  0.16180414\n",
      "epochs =  42 , step =  0 , loss_val =  0.18938214\n",
      "epochs =  42 , step =  100 , loss_val =  0.24631876\n",
      "epochs =  42 , step =  200 , loss_val =  0.1636376\n",
      "epochs =  42 , step =  300 , loss_val =  0.2570445\n",
      "epochs =  42 , step =  400 , loss_val =  0.18653114\n",
      "epochs =  42 , step =  500 , loss_val =  0.1859051\n",
      "epochs =  43 , step =  0 , loss_val =  0.19842127\n",
      "epochs =  43 , step =  100 , loss_val =  0.09114408\n",
      "epochs =  43 , step =  200 , loss_val =  0.19608353\n",
      "epochs =  43 , step =  300 , loss_val =  0.1708568\n",
      "epochs =  43 , step =  400 , loss_val =  0.23365779\n",
      "epochs =  43 , step =  500 , loss_val =  0.23550206\n",
      "epochs =  44 , step =  0 , loss_val =  0.16539648\n",
      "epochs =  44 , step =  100 , loss_val =  0.16513054\n",
      "epochs =  44 , step =  200 , loss_val =  0.15186945\n",
      "epochs =  44 , step =  300 , loss_val =  0.056545258\n",
      "epochs =  44 , step =  400 , loss_val =  0.12051821\n",
      "epochs =  44 , step =  500 , loss_val =  0.109227054\n",
      "epochs =  45 , step =  0 , loss_val =  0.06499504\n",
      "epochs =  45 , step =  100 , loss_val =  0.18812627\n",
      "epochs =  45 , step =  200 , loss_val =  0.25552243\n",
      "epochs =  45 , step =  300 , loss_val =  0.12762235\n",
      "epochs =  45 , step =  400 , loss_val =  0.122306526\n",
      "epochs =  45 , step =  500 , loss_val =  0.050054155\n",
      "epochs =  46 , step =  0 , loss_val =  0.21407291\n",
      "epochs =  46 , step =  100 , loss_val =  0.046617337\n",
      "epochs =  46 , step =  200 , loss_val =  0.09888888\n",
      "epochs =  46 , step =  300 , loss_val =  0.12767495\n",
      "epochs =  46 , step =  400 , loss_val =  0.07844331\n",
      "epochs =  46 , step =  500 , loss_val =  0.24908346\n",
      "epochs =  47 , step =  0 , loss_val =  0.120506436\n",
      "epochs =  47 , step =  100 , loss_val =  0.09445784\n",
      "epochs =  47 , step =  200 , loss_val =  0.17402415\n",
      "epochs =  47 , step =  300 , loss_val =  0.17585364\n",
      "epochs =  47 , step =  400 , loss_val =  0.17102447\n",
      "epochs =  47 , step =  500 , loss_val =  0.05370297\n",
      "epochs =  48 , step =  0 , loss_val =  0.29109073\n",
      "epochs =  48 , step =  100 , loss_val =  0.11489698\n",
      "epochs =  48 , step =  200 , loss_val =  0.19244386\n",
      "epochs =  48 , step =  300 , loss_val =  0.13159782\n",
      "epochs =  48 , step =  400 , loss_val =  0.112340935\n",
      "epochs =  48 , step =  500 , loss_val =  0.11239334\n",
      "epochs =  49 , step =  0 , loss_val =  0.2632698\n",
      "epochs =  49 , step =  100 , loss_val =  0.1472531\n",
      "epochs =  49 , step =  200 , loss_val =  0.2150182\n",
      "epochs =  49 , step =  300 , loss_val =  0.18804307\n",
      "epochs =  49 , step =  400 , loss_val =  0.17493248\n",
      "epochs =  49 , step =  500 , loss_val =  0.13689601\n",
      "\n",
      "Elapsed Time =>  0:01:05.165578\n",
      "\n",
      "\n",
      "Accuracy =  0.9458\n",
      "type(accuracy_val) =  <class 'numpy.float32'> , type(predicted_list_val) =  <class 'numpy.ndarray'> , type(index_label) =  <class 'numpy.ndarray'>\n",
      "index_label.shape =  (10000,)\n",
      "length of index_label_list =  10000\n",
      "false label count =  542\n",
      "\n",
      "length of index_label_false_list_1 542\n",
      "\n",
      "length of index_label_false_list_2 542\n"
     ]
    }
   ],
   "source": [
    "index_label_false_list_1 = []\n",
    "index_label_false_list_2 = []\n",
    "\n",
    "with  tf.Session()  as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())  # 변수 노드(tf.Variable) 초기화\n",
    "        \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    for i in range(epochs):    # 50 번 반복수행\n",
    "        \n",
    "        total_batch = int(mnist.train.num_examples / batch_size)  # 55,000 / 100\n",
    "\n",
    "        for step in range(total_batch):\n",
    "            \n",
    "            batch_x_data, batch_t_data = mnist.train.next_batch(batch_size)\n",
    "      \n",
    "            loss_val, _ = sess.run([loss, train], feed_dict={X: batch_x_data, T: batch_t_data})    \n",
    "        \n",
    "            if step % 100 == 0:\n",
    "                print(\"epochs = \", i, \", step = \", step, \", loss_val = \", loss_val)             \n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Elapsed Time => \", end_time-start_time)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Accuracy 확인\n",
    "    test_x_data = mnist.test.images    # 10000 X 784\n",
    "    test_t_data = mnist.test.labels    # 10000 X 10\n",
    "    \n",
    "    accuracy_val, predicted_list_val, index_label = sess.run([accuracy, predicted_list, accuracy_index], feed_dict={X: test_x_data, T: test_t_data})\n",
    "    \n",
    "    print(\"\\nAccuracy = \", accuracy_val)\n",
    "    print(\"type(accuracy_val) = \", type(accuracy_val), ', type(predicted_list_val) = ', type(predicted_list_val), ', type(index_label) = ', type(index_label))\n",
    "    print(\"index_label.shape = \", index_label.shape)\n",
    "    \n",
    "    index_label_list = list(index_label)\n",
    "    print(\"length of index_label_list = \", len(index_label_list))\n",
    "    print(\"false label count = \", index_label_list.count([0]))\n",
    "    \n",
    "    \n",
    "    # list type 으로 디버그\n",
    "    temp_list = [] \n",
    "    \n",
    "    for index in range(len(index_label_list)):\n",
    "        \n",
    "        if index_label_list[index] == 0:\n",
    "            \n",
    "            temp_list.append(index)\n",
    "            temp_list.append(np.argmax(test_t_data[index]))  # one-hot encoding 이므로 argmax 로 정답 추출\n",
    "            temp_list.append(predicted_list_val[index])\n",
    "            \n",
    "            index_label_false_list_1.append(temp_list)\n",
    "            \n",
    "            temp_list = []\n",
    "            \n",
    "    print(\"\\nlength of index_label_false_list_1\", len(index_label_false_list_1))\n",
    "    \n",
    "    # numpy type 으로 디버그\n",
    "    temp_list = [] \n",
    "    \n",
    "    for index in range(len(index_label)):\n",
    "        \n",
    "        if index_label[index] == 0:\n",
    "            \n",
    "            temp_list.append(index)\n",
    "            temp_list.append(np.argmax(test_t_data[index]))  # one-hot encoding 이므로 argmax 로 정답 추출\n",
    "            temp_list.append(predicted_list_val[index])\n",
    "            \n",
    "            index_label_false_list_2.append(temp_list)\n",
    "            \n",
    "            temp_list = []\n",
    "            \n",
    "    print(\"\\nlength of index_label_false_list_2\", len(index_label_false_list_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "dHSloPGA3dC1",
    "outputId": "10829e5e-d384-4d79-ed26-96d7eeaca127"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33, 4, 6], [38, 2, 3], [109, 4, 9], [115, 4, 9], [121, 4, 6], [126, 0, 9], [149, 2, 6], [151, 9, 3], [158, 3, 2], [187, 5, 3], [193, 9, 8], [241, 9, 8], [244, 2, 7], [247, 4, 2], [259, 6, 5], [264, 9, 8], [274, 9, 3], [290, 8, 4], [300, 4, 6], [320, 9, 7], [321, 2, 7], [325, 4, 8], [340, 5, 3], [341, 6, 4], [358, 7, 3], [445, 6, 0], [448, 9, 8], [449, 3, 5], [464, 3, 7], [478, 5, 8], [479, 9, 8], [495, 8, 0], [502, 5, 3], [507, 3, 5], [543, 8, 2], [565, 4, 9], [582, 8, 2], [591, 8, 3], [610, 4, 2], [613, 2, 8], [629, 2, 6], [646, 2, 6], [659, 2, 8], [667, 7, 2], [689, 7, 9], [691, 8, 4], [717, 0, 2], [720, 5, 8], [726, 7, 9], [740, 4, 9], [791, 5, 9], [810, 7, 2], [866, 5, 4], [877, 8, 6], [881, 4, 9], [890, 3, 5], [900, 1, 3], [924, 2, 7], [926, 2, 6], [938, 3, 5], [950, 7, 2], [951, 5, 4], [965, 6, 0], [966, 9, 8], [969, 4, 9], [1012, 7, 9], [1014, 6, 0], [1017, 6, 4], [1032, 5, 6], [1039, 7, 1], [1045, 9, 8], [1092, 3, 5], [1096, 7, 9], [1107, 9, 8], [1112, 4, 0], [1119, 7, 2], [1124, 8, 7], [1156, 7, 3], [1178, 4, 0], [1191, 0, 4], [1194, 7, 9], [1200, 8, 0], [1204, 3, 2], [1226, 7, 2], [1232, 9, 4], [1247, 9, 0], [1248, 8, 5], [1256, 2, 3], [1260, 7, 1], [1272, 5, 7], [1274, 4, 2], [1289, 5, 9], [1315, 3, 5], [1319, 8, 3], [1326, 7, 2], [1328, 7, 8], [1331, 5, 3], [1364, 8, 2], [1391, 4, 9], [1394, 8, 5], [1414, 9, 7], [1422, 4, 9], [1429, 9, 4], [1440, 4, 9], [1444, 6, 4], [1465, 4, 2], [1468, 0, 6], [1469, 3, 0], [1500, 7, 1], [1530, 8, 7], [1543, 7, 3], [1545, 9, 7], [1549, 4, 2], [1553, 9, 8], [1559, 9, 3], [1569, 6, 2], [1570, 0, 2], [1609, 2, 6], [1620, 2, 3], [1626, 6, 5], [1678, 2, 0], [1681, 3, 7], [1686, 8, 5], [1709, 9, 5], [1737, 5, 3], [1754, 7, 1], [1759, 8, 2], [1790, 2, 7], [1800, 6, 2], [1813, 8, 5], [1822, 6, 5], [1850, 8, 3], [1865, 4, 9], [1871, 2, 3], [1901, 9, 4], [1938, 4, 6], [1941, 7, 2], [1952, 9, 8], [1954, 5, 4], [1955, 8, 3], [1969, 6, 8], [1982, 6, 2], [1987, 0, 9], [2016, 7, 2], [2023, 0, 5], [2024, 7, 9], [2040, 5, 4], [2043, 4, 8], [2044, 2, 7], [2063, 7, 1], [2070, 7, 9], [2109, 3, 7], [2118, 6, 5], [2125, 5, 9], [2130, 4, 9], [2135, 6, 1], [2182, 1, 2], [2183, 4, 6], [2185, 0, 5], [2186, 2, 0], [2189, 9, 1], [2201, 4, 9], [2215, 6, 5], [2224, 5, 6], [2237, 5, 8], [2258, 1, 7], [2266, 1, 6], [2291, 5, 8], [2293, 9, 6], [2299, 2, 7], [2325, 7, 1], [2333, 0, 5], [2369, 5, 9], [2371, 4, 9], [2387, 9, 1], [2395, 8, 0], [2406, 9, 8], [2414, 9, 4], [2422, 6, 4], [2425, 8, 3], [2433, 2, 1], [2449, 0, 5], [2454, 6, 5], [2479, 0, 2], [2488, 2, 4], [2508, 6, 1], [2514, 4, 9], [2526, 5, 3], [2532, 6, 1], [2539, 7, 9], [2582, 9, 5], [2589, 9, 8], [2597, 5, 3], [2598, 8, 7], [2607, 7, 2], [2610, 2, 8], [2648, 9, 0], [2654, 6, 1], [2658, 4, 8], [2659, 4, 2], [2681, 3, 5], [2689, 5, 8], [2698, 5, 3], [2720, 9, 4], [2730, 7, 2], [2743, 5, 8], [2751, 6, 5], [2754, 6, 0], [2770, 3, 4], [2810, 5, 0], [2834, 8, 5], [2905, 8, 1], [2915, 7, 3], [2921, 3, 2], [2927, 3, 2], [2930, 5, 1], [2939, 9, 5], [2953, 3, 5], [2970, 5, 3], [2979, 9, 7], [2989, 3, 8], [3001, 9, 8], [3005, 9, 1], [3012, 8, 9], [3030, 6, 1], [3060, 9, 7], [3073, 1, 2], [3089, 2, 8], [3095, 5, 8], [3108, 3, 5], [3114, 4, 6], [3117, 5, 9], [3147, 9, 4], [3189, 7, 4], [3206, 8, 3], [3239, 2, 1], [3240, 9, 8], [3254, 6, 5], [3289, 8, 3], [3301, 2, 1], [3322, 0, 5], [3329, 7, 2], [3330, 2, 3], [3333, 7, 9], [3337, 2, 7], [3385, 9, 7], [3422, 6, 0], [3474, 2, 3], [3475, 3, 7], [3490, 4, 8], [3503, 9, 1], [3520, 6, 4], [3549, 3, 2], [3552, 5, 4], [3558, 5, 0], [3559, 8, 5], [3567, 8, 5], [3574, 0, 7], [3597, 9, 3], [3634, 0, 2], [3656, 7, 3], [3681, 2, 3], [3703, 2, 1], [3716, 9, 3], [3718, 4, 9], [3730, 7, 9], [3742, 3, 8], [3751, 7, 2], [3757, 8, 0], [3767, 7, 2], [3772, 2, 3], [3776, 5, 8], [3780, 4, 6], [3796, 2, 8], [3801, 6, 0], [3808, 7, 8], [3818, 0, 4], [3846, 6, 1], [3853, 6, 5], [3864, 0, 5], [3869, 9, 4], [3906, 1, 2], [3924, 9, 4], [3926, 9, 3], [3941, 4, 2], [3943, 3, 5], [3946, 2, 8], [3951, 8, 0], [3962, 3, 4], [3970, 9, 4], [3976, 7, 2], [3985, 9, 4], [3986, 2, 3], [4007, 7, 4], [4063, 6, 5], [4075, 8, 0], [4078, 9, 7], [4093, 9, 4], [4140, 8, 9], [4152, 5, 1], [4163, 9, 5], [4176, 2, 7], [4199, 7, 9], [4205, 2, 1], [4224, 9, 7], [4236, 5, 6], [4238, 7, 9], [4248, 2, 8], [4256, 3, 2], [4259, 9, 4], [4271, 5, 3], [4284, 9, 3], [4289, 2, 7], [4294, 9, 7], [4306, 3, 7], [4344, 9, 3], [4355, 5, 3], [4369, 9, 4], [4374, 5, 6], [4382, 4, 9], [4400, 7, 3], [4425, 9, 4], [4439, 6, 4], [4449, 6, 5], [4451, 2, 8], [4454, 9, 4], [4477, 0, 6], [4497, 8, 7], [4498, 7, 4], [4511, 9, 7], [4548, 5, 2], [4575, 4, 2], [4578, 7, 9], [4601, 8, 4], [4605, 3, 8], [4633, 9, 4], [4635, 3, 5], [4657, 3, 8], [4668, 2, 3], [4731, 8, 7], [4735, 9, 4], [4738, 4, 6], [4761, 9, 8], [4798, 6, 5], [4807, 8, 5], [4814, 6, 5], [4823, 9, 4], [4826, 4, 9], [4833, 3, 7], [4838, 6, 5], [4852, 8, 5], [4860, 4, 9], [4874, 9, 2], [4878, 2, 0], [4879, 8, 6], [4880, 0, 8], [4886, 7, 1], [4890, 8, 3], [4918, 9, 4], [4929, 4, 7], [4951, 1, 7], [4952, 6, 5], [4956, 8, 4], [4966, 7, 3], [4974, 4, 9], [4990, 3, 2], [5001, 9, 4], [5046, 3, 2], [5065, 8, 1], [5067, 3, 2], [5068, 4, 7], [5138, 8, 5], [5140, 3, 5], [5237, 3, 2], [5265, 6, 4], [5331, 1, 6], [5457, 1, 8], [5556, 4, 9], [5600, 7, 9], [5608, 5, 8], [5634, 2, 3], [5642, 1, 5], [5654, 7, 9], [5677, 4, 6], [5734, 3, 7], [5736, 6, 2], [5749, 8, 2], [5801, 6, 8], [5833, 5, 3], [5835, 7, 9], [5842, 4, 9], [5845, 7, 9], [5887, 7, 5], [5913, 5, 2], [5914, 7, 9], [5936, 4, 9], [5937, 5, 3], [5955, 3, 8], [5973, 3, 8], [5985, 5, 3], [5997, 5, 8], [6019, 4, 9], [6023, 3, 8], [6035, 2, 0], [6042, 5, 8], [6043, 5, 3], [6045, 3, 9], [6059, 3, 8], [6093, 2, 3], [6101, 1, 8], [6168, 9, 3], [6173, 9, 2], [6347, 8, 5], [6386, 5, 3], [6392, 5, 2], [6480, 2, 8], [6505, 9, 0], [6511, 3, 5], [6555, 8, 7], [6557, 0, 5], [6558, 6, 2], [6560, 9, 5], [6568, 9, 4], [6571, 9, 7], [6597, 0, 9], [6598, 5, 3], [6603, 8, 9], [6624, 3, 5], [6625, 8, 4], [6632, 9, 8], [6651, 0, 5], [6725, 8, 2], [6739, 3, 7], [6744, 2, 1], [6746, 5, 4], [6785, 2, 4], [6817, 9, 4], [6847, 6, 4], [6872, 4, 0], [6914, 9, 3], [6926, 6, 4], [7054, 2, 7], [7178, 5, 2], [7216, 0, 3], [7235, 2, 9], [7249, 2, 0], [7338, 4, 8], [7432, 7, 2], [7434, 4, 8], [7451, 5, 6], [7492, 2, 7], [7545, 8, 9], [7595, 3, 8], [7619, 2, 3], [7668, 6, 2], [7736, 9, 7], [7779, 5, 3], [7797, 5, 6], [7800, 3, 2], [7811, 1, 8], [7821, 3, 2], [7842, 5, 0], [7860, 6, 2], [7899, 1, 8], [7902, 7, 8], [7991, 9, 8], [8020, 1, 8], [8081, 4, 6], [8091, 2, 8], [8094, 2, 8], [8196, 6, 0], [8198, 2, 4], [8246, 3, 8], [8263, 3, 9], [8277, 3, 8], [8287, 6, 4], [8293, 3, 9], [8294, 8, 5], [8311, 6, 2], [8325, 0, 6], [8339, 8, 6], [8408, 8, 5], [8416, 4, 5], [8453, 5, 8], [8502, 5, 8], [8508, 3, 2], [8520, 4, 9], [8523, 9, 4], [8527, 4, 9], [8607, 3, 8], [9008, 4, 0], [9009, 7, 2], [9010, 2, 8], [9015, 7, 2], [9016, 0, 5], [9022, 3, 2], [9024, 7, 2], [9045, 7, 2], [9103, 4, 6], [9209, 2, 8], [9211, 4, 9], [9217, 3, 2], [9427, 5, 3], [9450, 8, 5], [9482, 5, 8], [9503, 5, 4], [9534, 7, 9], [9540, 1, 8], [9587, 9, 4], [9607, 6, 0], [9624, 3, 8], [9634, 0, 8], [9642, 9, 7], [9655, 3, 7], [9662, 3, 2], [9679, 6, 3], [9692, 9, 7], [9698, 6, 5], [9700, 2, 7], [9716, 2, 5], [9719, 5, 0], [9726, 2, 3], [9729, 5, 6], [9745, 4, 2], [9751, 2, 8], [9758, 3, 5], [9762, 3, 8], [9768, 2, 0], [9779, 2, 8], [9783, 4, 2], [9808, 9, 4], [9811, 2, 8], [9828, 3, 5], [9839, 2, 3], [9840, 3, 2], [9867, 2, 7], [9888, 6, 0], [9905, 3, 9], [9926, 8, 5], [9944, 3, 9], [9975, 3, 2], [9982, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "print(index_label_false_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "TDXsPhrE3dC9",
    "outputId": "d5311b2c-e392-4e64-bc28-48e5c6fa36a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33, 4, 6], [38, 2, 3], [109, 4, 9], [115, 4, 9], [121, 4, 6], [126, 0, 9], [149, 2, 6], [151, 9, 3], [158, 3, 2], [187, 5, 3], [193, 9, 8], [241, 9, 8], [244, 2, 7], [247, 4, 2], [259, 6, 5], [264, 9, 8], [274, 9, 3], [290, 8, 4], [300, 4, 6], [320, 9, 7], [321, 2, 7], [325, 4, 8], [340, 5, 3], [341, 6, 4], [358, 7, 3], [445, 6, 0], [448, 9, 8], [449, 3, 5], [464, 3, 7], [478, 5, 8], [479, 9, 8], [495, 8, 0], [502, 5, 3], [507, 3, 5], [543, 8, 2], [565, 4, 9], [582, 8, 2], [591, 8, 3], [610, 4, 2], [613, 2, 8], [629, 2, 6], [646, 2, 6], [659, 2, 8], [667, 7, 2], [689, 7, 9], [691, 8, 4], [717, 0, 2], [720, 5, 8], [726, 7, 9], [740, 4, 9], [791, 5, 9], [810, 7, 2], [866, 5, 4], [877, 8, 6], [881, 4, 9], [890, 3, 5], [900, 1, 3], [924, 2, 7], [926, 2, 6], [938, 3, 5], [950, 7, 2], [951, 5, 4], [965, 6, 0], [966, 9, 8], [969, 4, 9], [1012, 7, 9], [1014, 6, 0], [1017, 6, 4], [1032, 5, 6], [1039, 7, 1], [1045, 9, 8], [1092, 3, 5], [1096, 7, 9], [1107, 9, 8], [1112, 4, 0], [1119, 7, 2], [1124, 8, 7], [1156, 7, 3], [1178, 4, 0], [1191, 0, 4], [1194, 7, 9], [1200, 8, 0], [1204, 3, 2], [1226, 7, 2], [1232, 9, 4], [1247, 9, 0], [1248, 8, 5], [1256, 2, 3], [1260, 7, 1], [1272, 5, 7], [1274, 4, 2], [1289, 5, 9], [1315, 3, 5], [1319, 8, 3], [1326, 7, 2], [1328, 7, 8], [1331, 5, 3], [1364, 8, 2], [1391, 4, 9], [1394, 8, 5], [1414, 9, 7], [1422, 4, 9], [1429, 9, 4], [1440, 4, 9], [1444, 6, 4], [1465, 4, 2], [1468, 0, 6], [1469, 3, 0], [1500, 7, 1], [1530, 8, 7], [1543, 7, 3], [1545, 9, 7], [1549, 4, 2], [1553, 9, 8], [1559, 9, 3], [1569, 6, 2], [1570, 0, 2], [1609, 2, 6], [1620, 2, 3], [1626, 6, 5], [1678, 2, 0], [1681, 3, 7], [1686, 8, 5], [1709, 9, 5], [1737, 5, 3], [1754, 7, 1], [1759, 8, 2], [1790, 2, 7], [1800, 6, 2], [1813, 8, 5], [1822, 6, 5], [1850, 8, 3], [1865, 4, 9], [1871, 2, 3], [1901, 9, 4], [1938, 4, 6], [1941, 7, 2], [1952, 9, 8], [1954, 5, 4], [1955, 8, 3], [1969, 6, 8], [1982, 6, 2], [1987, 0, 9], [2016, 7, 2], [2023, 0, 5], [2024, 7, 9], [2040, 5, 4], [2043, 4, 8], [2044, 2, 7], [2063, 7, 1], [2070, 7, 9], [2109, 3, 7], [2118, 6, 5], [2125, 5, 9], [2130, 4, 9], [2135, 6, 1], [2182, 1, 2], [2183, 4, 6], [2185, 0, 5], [2186, 2, 0], [2189, 9, 1], [2201, 4, 9], [2215, 6, 5], [2224, 5, 6], [2237, 5, 8], [2258, 1, 7], [2266, 1, 6], [2291, 5, 8], [2293, 9, 6], [2299, 2, 7], [2325, 7, 1], [2333, 0, 5], [2369, 5, 9], [2371, 4, 9], [2387, 9, 1], [2395, 8, 0], [2406, 9, 8], [2414, 9, 4], [2422, 6, 4], [2425, 8, 3], [2433, 2, 1], [2449, 0, 5], [2454, 6, 5], [2479, 0, 2], [2488, 2, 4], [2508, 6, 1], [2514, 4, 9], [2526, 5, 3], [2532, 6, 1], [2539, 7, 9], [2582, 9, 5], [2589, 9, 8], [2597, 5, 3], [2598, 8, 7], [2607, 7, 2], [2610, 2, 8], [2648, 9, 0], [2654, 6, 1], [2658, 4, 8], [2659, 4, 2], [2681, 3, 5], [2689, 5, 8], [2698, 5, 3], [2720, 9, 4], [2730, 7, 2], [2743, 5, 8], [2751, 6, 5], [2754, 6, 0], [2770, 3, 4], [2810, 5, 0], [2834, 8, 5], [2905, 8, 1], [2915, 7, 3], [2921, 3, 2], [2927, 3, 2], [2930, 5, 1], [2939, 9, 5], [2953, 3, 5], [2970, 5, 3], [2979, 9, 7], [2989, 3, 8], [3001, 9, 8], [3005, 9, 1], [3012, 8, 9], [3030, 6, 1], [3060, 9, 7], [3073, 1, 2], [3089, 2, 8], [3095, 5, 8], [3108, 3, 5], [3114, 4, 6], [3117, 5, 9], [3147, 9, 4], [3189, 7, 4], [3206, 8, 3], [3239, 2, 1], [3240, 9, 8], [3254, 6, 5], [3289, 8, 3], [3301, 2, 1], [3322, 0, 5], [3329, 7, 2], [3330, 2, 3], [3333, 7, 9], [3337, 2, 7], [3385, 9, 7], [3422, 6, 0], [3474, 2, 3], [3475, 3, 7], [3490, 4, 8], [3503, 9, 1], [3520, 6, 4], [3549, 3, 2], [3552, 5, 4], [3558, 5, 0], [3559, 8, 5], [3567, 8, 5], [3574, 0, 7], [3597, 9, 3], [3634, 0, 2], [3656, 7, 3], [3681, 2, 3], [3703, 2, 1], [3716, 9, 3], [3718, 4, 9], [3730, 7, 9], [3742, 3, 8], [3751, 7, 2], [3757, 8, 0], [3767, 7, 2], [3772, 2, 3], [3776, 5, 8], [3780, 4, 6], [3796, 2, 8], [3801, 6, 0], [3808, 7, 8], [3818, 0, 4], [3846, 6, 1], [3853, 6, 5], [3864, 0, 5], [3869, 9, 4], [3906, 1, 2], [3924, 9, 4], [3926, 9, 3], [3941, 4, 2], [3943, 3, 5], [3946, 2, 8], [3951, 8, 0], [3962, 3, 4], [3970, 9, 4], [3976, 7, 2], [3985, 9, 4], [3986, 2, 3], [4007, 7, 4], [4063, 6, 5], [4075, 8, 0], [4078, 9, 7], [4093, 9, 4], [4140, 8, 9], [4152, 5, 1], [4163, 9, 5], [4176, 2, 7], [4199, 7, 9], [4205, 2, 1], [4224, 9, 7], [4236, 5, 6], [4238, 7, 9], [4248, 2, 8], [4256, 3, 2], [4259, 9, 4], [4271, 5, 3], [4284, 9, 3], [4289, 2, 7], [4294, 9, 7], [4306, 3, 7], [4344, 9, 3], [4355, 5, 3], [4369, 9, 4], [4374, 5, 6], [4382, 4, 9], [4400, 7, 3], [4425, 9, 4], [4439, 6, 4], [4449, 6, 5], [4451, 2, 8], [4454, 9, 4], [4477, 0, 6], [4497, 8, 7], [4498, 7, 4], [4511, 9, 7], [4548, 5, 2], [4575, 4, 2], [4578, 7, 9], [4601, 8, 4], [4605, 3, 8], [4633, 9, 4], [4635, 3, 5], [4657, 3, 8], [4668, 2, 3], [4731, 8, 7], [4735, 9, 4], [4738, 4, 6], [4761, 9, 8], [4798, 6, 5], [4807, 8, 5], [4814, 6, 5], [4823, 9, 4], [4826, 4, 9], [4833, 3, 7], [4838, 6, 5], [4852, 8, 5], [4860, 4, 9], [4874, 9, 2], [4878, 2, 0], [4879, 8, 6], [4880, 0, 8], [4886, 7, 1], [4890, 8, 3], [4918, 9, 4], [4929, 4, 7], [4951, 1, 7], [4952, 6, 5], [4956, 8, 4], [4966, 7, 3], [4974, 4, 9], [4990, 3, 2], [5001, 9, 4], [5046, 3, 2], [5065, 8, 1], [5067, 3, 2], [5068, 4, 7], [5138, 8, 5], [5140, 3, 5], [5237, 3, 2], [5265, 6, 4], [5331, 1, 6], [5457, 1, 8], [5556, 4, 9], [5600, 7, 9], [5608, 5, 8], [5634, 2, 3], [5642, 1, 5], [5654, 7, 9], [5677, 4, 6], [5734, 3, 7], [5736, 6, 2], [5749, 8, 2], [5801, 6, 8], [5833, 5, 3], [5835, 7, 9], [5842, 4, 9], [5845, 7, 9], [5887, 7, 5], [5913, 5, 2], [5914, 7, 9], [5936, 4, 9], [5937, 5, 3], [5955, 3, 8], [5973, 3, 8], [5985, 5, 3], [5997, 5, 8], [6019, 4, 9], [6023, 3, 8], [6035, 2, 0], [6042, 5, 8], [6043, 5, 3], [6045, 3, 9], [6059, 3, 8], [6093, 2, 3], [6101, 1, 8], [6168, 9, 3], [6173, 9, 2], [6347, 8, 5], [6386, 5, 3], [6392, 5, 2], [6480, 2, 8], [6505, 9, 0], [6511, 3, 5], [6555, 8, 7], [6557, 0, 5], [6558, 6, 2], [6560, 9, 5], [6568, 9, 4], [6571, 9, 7], [6597, 0, 9], [6598, 5, 3], [6603, 8, 9], [6624, 3, 5], [6625, 8, 4], [6632, 9, 8], [6651, 0, 5], [6725, 8, 2], [6739, 3, 7], [6744, 2, 1], [6746, 5, 4], [6785, 2, 4], [6817, 9, 4], [6847, 6, 4], [6872, 4, 0], [6914, 9, 3], [6926, 6, 4], [7054, 2, 7], [7178, 5, 2], [7216, 0, 3], [7235, 2, 9], [7249, 2, 0], [7338, 4, 8], [7432, 7, 2], [7434, 4, 8], [7451, 5, 6], [7492, 2, 7], [7545, 8, 9], [7595, 3, 8], [7619, 2, 3], [7668, 6, 2], [7736, 9, 7], [7779, 5, 3], [7797, 5, 6], [7800, 3, 2], [7811, 1, 8], [7821, 3, 2], [7842, 5, 0], [7860, 6, 2], [7899, 1, 8], [7902, 7, 8], [7991, 9, 8], [8020, 1, 8], [8081, 4, 6], [8091, 2, 8], [8094, 2, 8], [8196, 6, 0], [8198, 2, 4], [8246, 3, 8], [8263, 3, 9], [8277, 3, 8], [8287, 6, 4], [8293, 3, 9], [8294, 8, 5], [8311, 6, 2], [8325, 0, 6], [8339, 8, 6], [8408, 8, 5], [8416, 4, 5], [8453, 5, 8], [8502, 5, 8], [8508, 3, 2], [8520, 4, 9], [8523, 9, 4], [8527, 4, 9], [8607, 3, 8], [9008, 4, 0], [9009, 7, 2], [9010, 2, 8], [9015, 7, 2], [9016, 0, 5], [9022, 3, 2], [9024, 7, 2], [9045, 7, 2], [9103, 4, 6], [9209, 2, 8], [9211, 4, 9], [9217, 3, 2], [9427, 5, 3], [9450, 8, 5], [9482, 5, 8], [9503, 5, 4], [9534, 7, 9], [9540, 1, 8], [9587, 9, 4], [9607, 6, 0], [9624, 3, 8], [9634, 0, 8], [9642, 9, 7], [9655, 3, 7], [9662, 3, 2], [9679, 6, 3], [9692, 9, 7], [9698, 6, 5], [9700, 2, 7], [9716, 2, 5], [9719, 5, 0], [9726, 2, 3], [9729, 5, 6], [9745, 4, 2], [9751, 2, 8], [9758, 3, 5], [9762, 3, 8], [9768, 2, 0], [9779, 2, 8], [9783, 4, 2], [9808, 9, 4], [9811, 2, 8], [9828, 3, 5], [9839, 2, 3], [9840, 3, 2], [9867, 2, 7], [9888, 6, 0], [9905, 3, 9], [9926, 8, 5], [9944, 3, 9], [9975, 3, 2], [9982, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "print(index_label_false_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "Z4oFsr6r3dDB",
    "outputId": "c12d3a38-f2a7-4bef-a33c-1e8704ffb2e4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label =  3\n"
     ]
    }
   ],
   "source": [
    "# check false data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img = test_x_data[9944].reshape(28,28)  \n",
    "\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print(\"label = \", np.argmax(test_t_data[9944]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "Psd-5T1P3dDF",
    "outputId": "a102992d-bddc-4f6c-dd89-db2bd1012baf"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmHagea4WrdUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label =  9\n"
     ]
    }
   ],
   "source": [
    "# check false data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img = test_x_data[9926].reshape(28,28)  \n",
    "\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print(\"label = \", np.argmax(test_t_data[9918]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "w18K2SHS3dDK",
    "outputId": "08560819-a2c8-44dc-806d-baf28215b7d9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label =  5\n"
     ]
    }
   ],
   "source": [
    "# check false data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img = test_x_data[8].reshape(28,28)  \n",
    "\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print(\"label = \", np.argmax(test_t_data[8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tDVnJiRwixFO"
   },
   "source": [
    "colab 에서 drive mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "wO8oD6VUit4P",
    "outputId": "2d44ec0d-3c7f-4234-af8a-ebe1be26ecd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive   # colab 사용시 mount 하기 위한 용도. local 에서는 불필요\n",
    "drive.mount('/content/gdrive/')  # colab 사용시 mount 하기 위한 용도. local 에서는 불필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tTFqmkhM3dDR"
   },
   "source": [
    "#### 파일로 이미지 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MjFyDwAt3dDS",
    "outputId": "09654bec-e21d-4370-bbb9-9b0aba6f3573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 image is saved now\n",
      "20 image is saved now\n",
      "30 image is saved now\n",
      "40 image is saved now\n",
      "50 image is saved now\n",
      "60 image is saved now\n",
      "70 image is saved now\n",
      "80 image is saved now\n",
      "90 image is saved now\n",
      "100 image is saved now\n",
      "110 image is saved now\n",
      "120 image is saved now\n",
      "130 image is saved now\n",
      "140 image is saved now\n",
      "150 image is saved now\n",
      "160 image is saved now\n",
      "170 image is saved now\n",
      "180 image is saved now\n",
      "190 image is saved now\n",
      "200 image is saved now\n",
      "210 image is saved now\n",
      "220 image is saved now\n",
      "230 image is saved now\n",
      "240 image is saved now\n",
      "250 image is saved now\n",
      "260 image is saved now\n",
      "270 image is saved now\n",
      "280 image is saved now\n",
      "290 image is saved now\n",
      "300 image is saved now\n",
      "310 image is saved now\n",
      "320 image is saved now\n",
      "330 image is saved now\n",
      "340 image is saved now\n",
      "350 image is saved now\n",
      "360 image is saved now\n",
      "370 image is saved now\n",
      "380 image is saved now\n",
      "390 image is saved now\n",
      "400 image is saved now\n",
      "410 image is saved now\n",
      "420 image is saved now\n",
      "430 image is saved now\n",
      "440 image is saved now\n",
      "450 image is saved now\n",
      "460 image is saved now\n",
      "470 image is saved now\n",
      "480 image is saved now\n",
      "490 image is saved now\n",
      "500 image is saved now\n",
      "510 image is saved now\n",
      "520 image is saved now\n",
      "530 image is saved now\n",
      "540 image is saved now\n",
      "Elapsed save time =>  0:18:40.006539\n",
      "Total  542  data is saved\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check false data\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "save_count = 0\n",
    "\n",
    "# 현재 디렉토리 저장\n",
    "curr_dir = os.getcwd()\n",
    "\n",
    "# image 저장할 디렉토리 생성. 현재 시간으로 생성\n",
    "now = datetime.now()\n",
    "algorithm_name = 'NN1_100Hidden_GD_'\n",
    "save_dir_name = algorithm_name + str(now.year) + '-' + str(now.month) + '-' + str(now.day) + '_' + str(now.hour) + str(now.minute) + str(now.second)\n",
    "\n",
    "colab_default_dir = '/content/gdrive/My Drive/Colab Notebooks/'\n",
    "\n",
    "os.chdir(colab_default_dir)\n",
    "os.mkdir(save_dir_name)\n",
    "\n",
    "# change dir\n",
    "os.chdir(save_dir_name)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for list_data in index_label_false_list_2:\n",
    "    \n",
    "    index_int = list_data[0]\n",
    "    label_int = list_data[1]\n",
    "    prediction_int = list_data[2]\n",
    "        \n",
    "    # 저장할 이미지를 인덱스를 이용하여 가져옴\n",
    "    img = test_x_data[index_int].reshape(28,28)  \n",
    "    plt.imshow(img, cmap='gray')\n",
    "    \n",
    "    # 정답 문자열\n",
    "    label_str = str(label_int)\n",
    "    \n",
    "    # 예측값 문자열\n",
    "    prediction_str = str(prediction_int)\n",
    "    \n",
    "    # 정답과 오답을 나타내는 문자열\n",
    "    label_prediction_str = 'label = ' + label_str + '  ,  prediction = ' + prediction_str\n",
    "    \n",
    "    # 저장 파일 이름 생성, str(index_int).png\n",
    "    save_image_name = str(index_int) + '.png'\n",
    "    \n",
    "    plt.title(label_prediction_str)\n",
    "    plt.savefig(save_image_name)\n",
    "    \n",
    "    save_count += 1\n",
    "    \n",
    "    if save_count % 10 == 0:\n",
    "        \n",
    "        print(save_count, 'image is saved now')\n",
    "\n",
    "    \n",
    "end_time = datetime.now()\n",
    "\n",
    "print('Elapsed save time => ', end_time - start_time)\n",
    "print('Total ', save_count, \" data is saved\")\n",
    "\n",
    "# 원래의 dir 로 복귀\n",
    "os.chdir(curr_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "q8DXozIy3dDa",
    "outputId": "c40bc8ff-7d5f-40a8-ab5b-a4aea2ea948d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "print(curr_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3Yz46CcplnA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "(191123)tensorflow_MNIST_debug_for_class.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
