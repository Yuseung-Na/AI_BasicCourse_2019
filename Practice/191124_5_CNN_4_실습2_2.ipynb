{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-baadd58151ab>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\YUSEUNG\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\YUSEUNG\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\YUSEUNG\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\YUSEUNG\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\YUSEUNG\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "\n",
      "train.num = 55000 , test.num = 10000 , validation.num = 5000\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "print(\"\")\n",
    "print(\"train.num =\", mnist.train.num_examples,\n",
    "     \", test.num =\", mnist.test.num_examples,\n",
    "     \", validation.num =\", mnist.validation.num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyper parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameter\n",
    "learning_rate = 1e-3\n",
    "epochs = 30\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력과 정답을 위한 플레이스홀더 정의\n",
    "X = tf.placeholder(tf.float32, [None, 784])  \n",
    "T = tf.placeholder(tf.float32, [None, 10])  \n",
    "\n",
    "# 입력층의 출력 값. 컨볼루션 연산을 위해 reshape 시킴\n",
    "A1 = X_img = tf.reshape(X, [-1, 28, 28, 1])  # image 28 x 28 x 1 (black / white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컨볼루션층 1 5x5 크기의 128개 필터로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1번째 컨볼루션 층, 5x5x128 필터 \n",
    "W2 = tf.Variable(tf.random_normal([5, 5, 1, 128], stddev=0.01))  # standard deviation 표준편차 0.01 이내로 뽑음 -> 더 정교한 데이터\n",
    "b2 = tf.Variable(tf.random_normal([128]))\n",
    "\n",
    "# 1번째 컨볼루션 연산을 통해 28 x 28 x 1 => 28 x 28 x 128  흑백인 1개 층이 -> 128개 층을 거치게 됨\n",
    "C2 = tf.nn.conv2d(A1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# relu\n",
    "Z2 = tf.nn.relu(C2+b2)\n",
    "\n",
    "# 1번째 max pooling을 통해 28 x 28 x 128 => 14 x 14 x 128  max pooling을 통해 4개를 1개로 묶어 가로 세로 2배씩 줄어들게 됨\n",
    "A2 = P2 = tf.nn.max_pool(Z2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 완전연결층 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14 x 14 크기를 가진 32개의 activation map을 flatten 시킴\n",
    "A2_flat = P2_flat = tf.reshape(A2, [-1, 14*14*128])  # 행렬로 변환, -1 : 행은 상관 없고, 14*14*32 : 열은 이거를 만족하게 만들어야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출력층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력층\n",
    "W3 = tf.Variable(tf.random_normal([14*14*128, 10], stddev=0.01))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# 출력층 선형회귀 값 Z3, 즉 softmax 에 들어가는 입력 값\n",
    "Z3 = logits = tf.matmul(A2_flat, W3) + b3\n",
    "\n",
    "y = A3 = tf.nn.softmax(Z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = Z3, labels = T))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)  # GradientDecent 방식을 사용하면 정답률이 91 정도로 떨어짐\n",
    "## Adam(momentum)방식이 mnist 데이터에 잘 맞게 tf가 최적화 되어 있음.\n",
    "\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size x 10 데이터에 대해 argmax를 통해 행단위로 비교함\n",
    "predicted_val = tf.equal(tf.argmax(A3, 1), tf.argmax(T, 1))\n",
    "\n",
    "# batch_size x 10 의 True, False 를 1 또는 0 으로 변환\n",
    "accuracy = tf.reduce_mean(tf.cast(predicted_val, dtype=tf.float32))\n",
    "\n",
    "accuracy_index = tf.cast(predicted_val, dtype=tf.float32)\n",
    "\n",
    "predicted_list = tf.argmax(A3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  0 , step =  0 , loss_val =  3.2188709\n",
      "epochs =  0 , step =  100 , loss_val =  0.69338655\n",
      "epochs =  0 , step =  200 , loss_val =  0.46189108\n",
      "epochs =  0 , step =  300 , loss_val =  0.33188465\n",
      "epochs =  0 , step =  400 , loss_val =  0.21028854\n",
      "epochs =  0 , step =  500 , loss_val =  0.23892282\n",
      "epochs =  1 , step =  0 , loss_val =  0.2903001\n",
      "epochs =  1 , step =  100 , loss_val =  0.23478727\n",
      "epochs =  1 , step =  200 , loss_val =  0.07630159\n",
      "epochs =  1 , step =  300 , loss_val =  0.18351811\n",
      "epochs =  1 , step =  400 , loss_val =  0.10681834\n",
      "epochs =  1 , step =  500 , loss_val =  0.06879854\n",
      "epochs =  2 , step =  0 , loss_val =  0.14692444\n",
      "epochs =  2 , step =  100 , loss_val =  0.02326676\n",
      "epochs =  2 , step =  200 , loss_val =  0.06530914\n",
      "epochs =  2 , step =  300 , loss_val =  0.09805751\n",
      "epochs =  2 , step =  400 , loss_val =  0.04555264\n",
      "epochs =  2 , step =  500 , loss_val =  0.15371126\n",
      "epochs =  3 , step =  0 , loss_val =  0.08188495\n",
      "epochs =  3 , step =  100 , loss_val =  0.031941537\n",
      "epochs =  3 , step =  200 , loss_val =  0.09062893\n",
      "epochs =  3 , step =  300 , loss_val =  0.072202794\n",
      "epochs =  3 , step =  400 , loss_val =  0.41174847\n",
      "epochs =  3 , step =  500 , loss_val =  0.2098204\n",
      "epochs =  4 , step =  0 , loss_val =  0.051777247\n",
      "epochs =  4 , step =  100 , loss_val =  0.19648713\n",
      "epochs =  4 , step =  200 , loss_val =  0.07501089\n",
      "epochs =  4 , step =  300 , loss_val =  0.09729908\n",
      "epochs =  4 , step =  400 , loss_val =  0.018746665\n",
      "epochs =  4 , step =  500 , loss_val =  0.019358406\n",
      "epochs =  5 , step =  0 , loss_val =  0.039992277\n",
      "epochs =  5 , step =  100 , loss_val =  0.12383309\n",
      "epochs =  5 , step =  200 , loss_val =  0.021567492\n",
      "epochs =  5 , step =  300 , loss_val =  0.07919236\n",
      "epochs =  5 , step =  400 , loss_val =  0.051227994\n",
      "epochs =  5 , step =  500 , loss_val =  0.043664716\n",
      "epochs =  6 , step =  0 , loss_val =  0.03749193\n",
      "epochs =  6 , step =  100 , loss_val =  0.17021526\n",
      "epochs =  6 , step =  200 , loss_val =  0.007142318\n",
      "epochs =  6 , step =  300 , loss_val =  0.016552212\n",
      "epochs =  6 , step =  400 , loss_val =  0.00077351584\n",
      "epochs =  6 , step =  500 , loss_val =  0.08453496\n",
      "epochs =  7 , step =  0 , loss_val =  0.055161443\n",
      "epochs =  7 , step =  100 , loss_val =  0.02342763\n",
      "epochs =  7 , step =  200 , loss_val =  0.10294489\n",
      "epochs =  7 , step =  300 , loss_val =  0.02447847\n",
      "epochs =  7 , step =  400 , loss_val =  0.06593075\n",
      "epochs =  7 , step =  500 , loss_val =  0.010324773\n",
      "epochs =  8 , step =  0 , loss_val =  0.018564291\n",
      "epochs =  8 , step =  100 , loss_val =  0.03695117\n",
      "epochs =  8 , step =  200 , loss_val =  0.053770795\n",
      "epochs =  8 , step =  300 , loss_val =  0.009849382\n",
      "epochs =  8 , step =  400 , loss_val =  0.0870069\n",
      "epochs =  8 , step =  500 , loss_val =  0.11672484\n",
      "epochs =  9 , step =  0 , loss_val =  0.019214148\n",
      "epochs =  9 , step =  100 , loss_val =  0.021709118\n",
      "epochs =  9 , step =  200 , loss_val =  0.043517366\n",
      "epochs =  9 , step =  300 , loss_val =  0.018802742\n",
      "epochs =  9 , step =  400 , loss_val =  0.039080795\n",
      "epochs =  9 , step =  500 , loss_val =  0.026197359\n",
      "epochs =  10 , step =  0 , loss_val =  0.030606063\n",
      "epochs =  10 , step =  100 , loss_val =  0.026771063\n",
      "epochs =  10 , step =  200 , loss_val =  0.02186627\n",
      "epochs =  10 , step =  300 , loss_val =  0.013978278\n",
      "epochs =  10 , step =  400 , loss_val =  0.039056025\n",
      "epochs =  10 , step =  500 , loss_val =  0.0135238\n",
      "epochs =  11 , step =  0 , loss_val =  0.0036053956\n",
      "epochs =  11 , step =  100 , loss_val =  0.0057069217\n",
      "epochs =  11 , step =  200 , loss_val =  0.0022543974\n",
      "epochs =  11 , step =  300 , loss_val =  0.02551085\n",
      "epochs =  11 , step =  400 , loss_val =  0.004104299\n",
      "epochs =  11 , step =  500 , loss_val =  0.01885147\n",
      "epochs =  12 , step =  0 , loss_val =  0.013222601\n",
      "epochs =  12 , step =  100 , loss_val =  0.01358226\n",
      "epochs =  12 , step =  200 , loss_val =  0.005794053\n",
      "epochs =  12 , step =  300 , loss_val =  0.010263005\n",
      "epochs =  12 , step =  400 , loss_val =  0.05661193\n",
      "epochs =  12 , step =  500 , loss_val =  0.0278969\n",
      "epochs =  13 , step =  0 , loss_val =  0.011207205\n",
      "epochs =  13 , step =  100 , loss_val =  0.0047472552\n",
      "epochs =  13 , step =  200 , loss_val =  0.034252137\n",
      "epochs =  13 , step =  300 , loss_val =  0.034919318\n",
      "epochs =  13 , step =  400 , loss_val =  0.033494826\n",
      "epochs =  13 , step =  500 , loss_val =  0.002848976\n",
      "epochs =  14 , step =  0 , loss_val =  0.0030424667\n",
      "epochs =  14 , step =  100 , loss_val =  0.012342512\n",
      "epochs =  14 , step =  200 , loss_val =  0.033647045\n",
      "epochs =  14 , step =  300 , loss_val =  0.0131658325\n",
      "epochs =  14 , step =  400 , loss_val =  0.016920453\n",
      "epochs =  14 , step =  500 , loss_val =  0.069632344\n",
      "epochs =  15 , step =  0 , loss_val =  0.0073216665\n",
      "epochs =  15 , step =  100 , loss_val =  0.015546102\n",
      "epochs =  15 , step =  200 , loss_val =  0.014058466\n",
      "epochs =  15 , step =  300 , loss_val =  0.011847557\n",
      "epochs =  15 , step =  400 , loss_val =  0.0099517545\n",
      "epochs =  15 , step =  500 , loss_val =  0.011232624\n",
      "epochs =  16 , step =  0 , loss_val =  0.07353242\n",
      "epochs =  16 , step =  100 , loss_val =  0.001634615\n",
      "epochs =  16 , step =  200 , loss_val =  0.005632451\n",
      "epochs =  16 , step =  300 , loss_val =  0.013326841\n",
      "epochs =  16 , step =  400 , loss_val =  0.03201859\n",
      "epochs =  16 , step =  500 , loss_val =  0.0023273695\n",
      "epochs =  17 , step =  0 , loss_val =  0.0048483573\n",
      "epochs =  17 , step =  100 , loss_val =  0.006347991\n",
      "epochs =  17 , step =  200 , loss_val =  0.016606396\n",
      "epochs =  17 , step =  300 , loss_val =  0.004551141\n",
      "epochs =  17 , step =  400 , loss_val =  0.0012743488\n",
      "epochs =  17 , step =  500 , loss_val =  0.0026029544\n",
      "epochs =  18 , step =  0 , loss_val =  0.004365952\n",
      "epochs =  18 , step =  100 , loss_val =  0.0007614368\n",
      "epochs =  18 , step =  200 , loss_val =  0.002197022\n",
      "epochs =  18 , step =  300 , loss_val =  0.010305095\n",
      "epochs =  18 , step =  400 , loss_val =  0.005572521\n",
      "epochs =  18 , step =  500 , loss_val =  0.009685153\n",
      "epochs =  19 , step =  0 , loss_val =  0.003279389\n",
      "epochs =  19 , step =  100 , loss_val =  0.00460171\n",
      "epochs =  19 , step =  200 , loss_val =  0.00077702646\n",
      "epochs =  19 , step =  300 , loss_val =  0.0051499857\n",
      "epochs =  19 , step =  400 , loss_val =  0.016611982\n",
      "epochs =  19 , step =  500 , loss_val =  0.013329408\n",
      "epochs =  20 , step =  0 , loss_val =  0.0033763417\n",
      "epochs =  20 , step =  100 , loss_val =  0.009515991\n",
      "epochs =  20 , step =  200 , loss_val =  0.0030988974\n",
      "epochs =  20 , step =  300 , loss_val =  0.009102967\n",
      "epochs =  20 , step =  400 , loss_val =  0.0030297223\n",
      "epochs =  20 , step =  500 , loss_val =  0.008165865\n",
      "epochs =  21 , step =  0 , loss_val =  0.0020421054\n",
      "epochs =  21 , step =  100 , loss_val =  0.0022822828\n",
      "epochs =  21 , step =  200 , loss_val =  0.025322333\n",
      "epochs =  21 , step =  300 , loss_val =  0.0020830524\n",
      "epochs =  21 , step =  400 , loss_val =  0.0033357912\n",
      "epochs =  21 , step =  500 , loss_val =  0.002468789\n",
      "epochs =  22 , step =  0 , loss_val =  0.0037340524\n",
      "epochs =  22 , step =  100 , loss_val =  0.0017891049\n",
      "epochs =  22 , step =  200 , loss_val =  0.0010327174\n",
      "epochs =  22 , step =  300 , loss_val =  0.004411043\n",
      "epochs =  22 , step =  400 , loss_val =  0.001402187\n",
      "epochs =  22 , step =  500 , loss_val =  0.007567785\n",
      "epochs =  23 , step =  0 , loss_val =  0.003797279\n",
      "epochs =  23 , step =  100 , loss_val =  0.0028716174\n",
      "epochs =  23 , step =  200 , loss_val =  0.005562269\n",
      "epochs =  23 , step =  300 , loss_val =  0.0052815406\n",
      "epochs =  23 , step =  400 , loss_val =  0.0005752596\n",
      "epochs =  23 , step =  500 , loss_val =  0.00079475634\n",
      "epochs =  24 , step =  0 , loss_val =  0.008102209\n",
      "epochs =  24 , step =  100 , loss_val =  0.0041445456\n",
      "epochs =  24 , step =  200 , loss_val =  0.0014498915\n",
      "epochs =  24 , step =  300 , loss_val =  0.011476985\n",
      "epochs =  24 , step =  400 , loss_val =  0.0009629945\n",
      "epochs =  24 , step =  500 , loss_val =  0.027514962\n",
      "epochs =  25 , step =  0 , loss_val =  0.007850888\n",
      "epochs =  25 , step =  100 , loss_val =  0.0038436132\n",
      "epochs =  25 , step =  200 , loss_val =  0.0027302557\n",
      "epochs =  25 , step =  300 , loss_val =  0.0016607955\n",
      "epochs =  25 , step =  400 , loss_val =  0.002089123\n",
      "epochs =  25 , step =  500 , loss_val =  0.04752832\n",
      "epochs =  26 , step =  0 , loss_val =  0.0014688985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  26 , step =  100 , loss_val =  0.0010257057\n",
      "epochs =  26 , step =  200 , loss_val =  0.0013052777\n",
      "epochs =  26 , step =  300 , loss_val =  0.016100476\n",
      "epochs =  26 , step =  400 , loss_val =  0.004486833\n",
      "epochs =  26 , step =  500 , loss_val =  0.0014385837\n",
      "epochs =  27 , step =  0 , loss_val =  0.0013394669\n",
      "epochs =  27 , step =  100 , loss_val =  0.00016994038\n",
      "epochs =  27 , step =  200 , loss_val =  0.00011387156\n",
      "epochs =  27 , step =  300 , loss_val =  0.000535998\n",
      "epochs =  27 , step =  400 , loss_val =  0.0009154407\n",
      "epochs =  27 , step =  500 , loss_val =  0.00012017638\n",
      "epochs =  28 , step =  0 , loss_val =  0.0012163682\n",
      "epochs =  28 , step =  100 , loss_val =  0.0017730831\n",
      "epochs =  28 , step =  200 , loss_val =  0.00048742007\n",
      "epochs =  28 , step =  300 , loss_val =  0.0013117662\n",
      "epochs =  28 , step =  400 , loss_val =  0.0005871617\n",
      "epochs =  28 , step =  500 , loss_val =  0.0013702601\n",
      "epochs =  29 , step =  0 , loss_val =  0.0020930727\n",
      "epochs =  29 , step =  100 , loss_val =  0.0011122861\n",
      "epochs =  29 , step =  200 , loss_val =  0.004308571\n",
      "epochs =  29 , step =  300 , loss_val =  0.0014962537\n",
      "epochs =  29 , step =  400 , loss_val =  0.0006789672\n",
      "epochs =  29 , step =  500 , loss_val =  0.00018676765\n",
      "\n",
      "Elapsed Time =>  0:38:15.660465\n",
      "\n",
      "Accuracy = 0.986\n",
      "length of index_label_list =  10000\n",
      "false label count =  140\n",
      "\n",
      "length of index_label_false_list_1 140\n"
     ]
    }
   ],
   "source": [
    "index_label_false_list_1 = []\n",
    "\n",
    "with tf.Session() as sess:  # with를 쓰면 close 하지 않아도 됨\n",
    "    sess.run(tf.global_variables_initializer())  # 변수 노드 (tf.Variable) 초기화\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    for i in range(epochs):  # 30번 반복 수행\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)  # 55000 / 100\n",
    "        \n",
    "        for step in range(total_batch):\n",
    "            batch_x_data, batch_t_data = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            loss_val, _ = sess.run([loss, train], feed_dict={X: batch_x_data, T: batch_t_data})    \n",
    "        \n",
    "            if step % 100 == 0:\n",
    "                print(\"epochs = \", i, \", step = \", step, \", loss_val = \", loss_val)             \n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Elapsed Time => \", end_time-start_time)\n",
    "    \n",
    "    # Accuracy 확인\n",
    "    test_x_data = mnist.test.images  # 10000 x 784\n",
    "    test_t_data = mnist.test.labels  # 10000 x 10\n",
    "    \n",
    "    accuracy_val, predicted_list_val, index_label = sess.run([accuracy, predicted_list, accuracy_index], feed_dict={X: test_x_data, T: test_t_data})\n",
    "    \n",
    "    print(\"\\nAccuracy =\", accuracy_val)\n",
    "    \n",
    "    index_label_list = list(index_label)\n",
    "    print(\"length of index_label_list = \", len(index_label_list))\n",
    "    print(\"false label count = \", index_label_list.count([0]))\n",
    "    \n",
    "    # list type 으로 디버그\n",
    "    temp_list = [] \n",
    "    \n",
    "    for index in range(len(index_label_list)):\n",
    "        \n",
    "        if index_label_list[index] == 0:\n",
    "            \n",
    "            temp_list.append(index)\n",
    "            temp_list.append(np.argmax(test_t_data[index]))  # one-hot encoding 이므로 argmax 로 정답 추출\n",
    "            temp_list.append(predicted_list_val[index])\n",
    "            \n",
    "            index_label_false_list_1.append(temp_list)\n",
    "            \n",
    "            temp_list = []\n",
    "            \n",
    "    print(\"\\nlength of index_label_false_list_1\", len(index_label_false_list_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[211, 5, 3], [259, 6, 0], [321, 2, 7], [340, 5, 3], [582, 8, 2], [583, 2, 7], [659, 2, 1], [674, 5, 3], [684, 7, 3], [740, 4, 9], [839, 8, 3], [844, 8, 7], [947, 8, 9], [1014, 6, 5], [1039, 7, 3], [1112, 4, 6], [1226, 7, 2], [1232, 9, 4], [1242, 4, 9], [1247, 9, 3], [1260, 7, 1], [1296, 6, 5], [1319, 8, 0], [1326, 7, 2], [1393, 5, 3], [1414, 9, 3], [1530, 8, 7], [1553, 9, 3], [1621, 0, 6], [1641, 5, 6], [1709, 9, 3], [1717, 8, 0], [1737, 5, 3], [1754, 7, 2], [1790, 2, 7], [1878, 8, 3], [1901, 9, 8], [2004, 8, 9], [2035, 5, 3], [2098, 2, 0], [2118, 6, 0], [2129, 9, 8], [2130, 4, 9], [2135, 6, 1], [2189, 9, 1], [2224, 5, 3], [2293, 9, 4], [2329, 0, 2], [2369, 5, 3], [2406, 9, 4], [2514, 4, 9], [2582, 9, 3], [2597, 5, 3], [2648, 9, 5], [2654, 6, 1], [2758, 8, 3], [2760, 9, 4], [2778, 4, 0], [2810, 5, 3], [2896, 8, 0], [2930, 5, 7], [2939, 9, 5], [2970, 5, 3], [2979, 9, 0], [3030, 6, 0], [3060, 9, 3], [3384, 2, 6], [3422, 6, 0], [3441, 7, 8], [3503, 9, 1], [3520, 6, 4], [3558, 5, 0], [3559, 8, 5], [3626, 8, 3], [3727, 8, 9], [3749, 6, 0], [3751, 7, 2], [3778, 5, 2], [3808, 7, 8], [3853, 6, 8], [3859, 9, 2], [4075, 8, 0], [4078, 9, 3], [4176, 2, 7], [4201, 1, 7], [4238, 7, 3], [4248, 2, 1], [4256, 3, 2], [4355, 5, 3], [4360, 5, 3], [4497, 8, 7], [4639, 8, 9], [4740, 3, 5], [4783, 4, 9], [4807, 8, 0], [4823, 9, 4], [5228, 6, 4], [5246, 7, 2], [5265, 6, 4], [5634, 2, 8], [5676, 4, 1], [5749, 8, 2], [5937, 5, 3], [5955, 3, 8], [5972, 5, 3], [5981, 5, 3], [5982, 5, 3], [5997, 5, 3], [6053, 5, 3], [6071, 9, 3], [6081, 9, 3], [6091, 9, 5], [6166, 9, 3], [6168, 9, 3], [6173, 9, 8], [6560, 9, 5], [6571, 9, 3], [6574, 2, 6], [6576, 7, 1], [6597, 0, 7], [6783, 1, 6], [8059, 2, 1], [8325, 0, 6], [8522, 8, 2], [8527, 4, 9], [9015, 7, 2], [9530, 9, 8], [9540, 1, 8], [9620, 9, 5], [9634, 0, 1], [9664, 2, 7], [9679, 6, 4], [9692, 9, 7], [9698, 6, 1], [9729, 5, 6], [9749, 5, 6], [9754, 5, 0], [9770, 5, 0], [9792, 4, 7], [9891, 9, 3]]\n"
     ]
    }
   ],
   "source": [
    "print(index_label_false_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
